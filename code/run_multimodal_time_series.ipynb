{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "\n",
    "import argparse\n",
    "from collections import namedtuple, OrderedDict\n",
    "import itertools\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "from typing import List\n",
    "from typing import Dict\n",
    "import random\n",
    "from itertools import product\n",
    "import copy\n",
    "import re\n",
    "import random\n",
    "import hashlib\n",
    "import pathlib\n",
    "import json\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "\n",
    "import logging\n",
    "\n",
    "preprocess = {\n",
    "    'acoustic': lambda df : df.loc[:,' F0semitoneFrom27.5Hz_sma3nz_amean':' equivalentSoundLevel_dBp'],\n",
    "    'acoustic_timer': lambda df : df.loc[:,' frameTime'],\n",
    "    'linguistic': lambda df : df.loc[:,'word'],\n",
    "    'linguistic_timer': lambda df : df.loc[:,'time-offset'],\n",
    "    'target': lambda df : df.loc[:,' rating'],\n",
    "    'target_timer': lambda df : df.loc[:,'time'],\n",
    "}\n",
    "\n",
    "class InputFeature:\n",
    "    \n",
    "    def __init__(\n",
    "        self, video_id=\"\",\n",
    "        acoustic_feature=[],\n",
    "        linguistic_feature=[],\n",
    "        visual_feature=[],\n",
    "        labels=[],\n",
    "    ):\n",
    "        self.video_id = video_id\n",
    "        self.acoustic_feature = acoustic_feature\n",
    "        self.linguistic_feature = linguistic_feature\n",
    "        self.visual_feature = visual_feature\n",
    "        self.labels = labels\n",
    "        \n",
    "def preprocess_SEND_files(\n",
    "    data_dir, # Multitmodal X\n",
    "    target_data_dir, # Y\n",
    "    time_window_in_sec=5.0,\n",
    "    modality_dir_map = {\"acoustic\": \"acoustic-egemaps\",  \n",
    "                        \"linguistic\": \"linguistic-word-level\", # we don't load features\n",
    "                        \"visual\": \"image-raw\", # image is nested,\n",
    "                        \"target\": \"target\",\n",
    "                       },\n",
    "    linguistic_tokenizer=None,\n",
    "    pad_symbol=0,\n",
    "):\n",
    "    SEND_videos = []\n",
    "    \n",
    "    # basically, let us gett all the video ids?\n",
    "    a_ids = [f.split(\"_\")[0]+\"_\"+f.split(\"_\")[1] \n",
    "             for f in listdir(os.path.join(data_dir, modality_dir_map[\"acoustic\"])) \n",
    "             if isfile(os.path.join(data_dir, modality_dir_map[\"acoustic\"], f))]\n",
    "    l_ids = [f.split(\"_\")[0]+\"_\"+f.split(\"_\")[1] \n",
    "             for f in listdir(os.path.join(data_dir, modality_dir_map[\"linguistic\"])) \n",
    "             if isfile(os.path.join(data_dir, modality_dir_map[\"linguistic\"], f))]\n",
    "    v_ids = [f.split(\"_\")[0]+\"_\"+f.split(\"_\")[1] \n",
    "             for f in listdir(os.path.join(data_dir, modality_dir_map[\"visual\"])) \n",
    "             if f != \".DS_Store\"]\n",
    "    assert len(a_ids) == len(l_ids) and len(l_ids) == len(v_ids)\n",
    "    assert len(set(a_ids).intersection(set(l_ids))) == len(l_ids)\n",
    "    assert len(set(a_ids).intersection(set(v_ids))) == len(v_ids)\n",
    "    \n",
    "    # We need the first pass for linguistic modality process?\n",
    "    max_window_l_length = -1\n",
    "    for video_id in a_ids: # pick any one!\n",
    "        # linguistic features process\n",
    "        l_file = os.path.join(data_dir, modality_dir_map[\"linguistic\"], f\"{video_id}_aligned.tsv\")\n",
    "        l_df = pd.read_csv(l_file, sep='\\t')\n",
    "        l_words = np.array(preprocess[\"linguistic\"](l_df))\n",
    "        l_words = [w.strip().lower() for w in l_words]\n",
    "        l_timestamps = np.array(preprocess[\"linguistic_timer\"](l_df))\n",
    "        # sample based on interval\n",
    "        current_time = 0.0\n",
    "        keep_first = True\n",
    "        sampled_l_words = [] # different from other modality, it is essentially a list of list!\n",
    "        tmp_words = []\n",
    "        for i in range(0, l_timestamps.shape[0]):\n",
    "            if keep_first:\n",
    "                sampled_l_words += [[]]\n",
    "                keep_first = False\n",
    "            if l_timestamps[i] >= current_time+time_window_in_sec:\n",
    "                sampled_l_words.append(tmp_words)\n",
    "                tmp_words = [l_words[i]] # reinit the buffer\n",
    "                current_time += time_window_in_sec\n",
    "                continue\n",
    "            tmp_words += [l_words[i]]\n",
    "        # overflow\n",
    "        if len(tmp_words) > 0:\n",
    "            sampled_l_words.append(tmp_words)\n",
    "        for window_words in sampled_l_words:\n",
    "            token_ids = linguistic_tokenizer.convert_tokens_to_ids(window_words)\n",
    "            if len(token_ids) > max_window_l_length:\n",
    "                max_window_l_length = len(token_ids)\n",
    "    max_window_l_length += 2 # the start and the end token\n",
    "    \n",
    "    max_seq_len = -1\n",
    "    video_count = 0\n",
    "    for video_id in a_ids: # pick any one!\n",
    "        if video_count > 1 and video_count%100 == 0:\n",
    "            logger.info(f\"Processed #{len(SEND_videos)} videos.\")\n",
    "            logger.info(SEND_videos[-1])\n",
    "        \n",
    "        # we need to fix this to get features aligned.\n",
    "        \n",
    "        # Step 1: Load rating data, and we can get window partitioned according to our interval.\n",
    "        target_id = video_id.split(\"_\")[0][2:] + \"_\" + video_id.split(\"_\")[1][3:]\n",
    "        target_file = os.path.join(target_data_dir, modality_dir_map[\"target\"], f\"target_{target_id}_normal.csv\")\n",
    "        target_df = pd.read_csv(target_file)\n",
    "        target_ratings = np.array(preprocess[\"target\"](target_df))\n",
    "        target_timestamps = np.array(preprocess[\"target_timer\"](target_df))\n",
    "        assert target_ratings.shape[0] == target_timestamps.shape[0]\n",
    "        windows = []\n",
    "        number_of_window = int(max(target_timestamps)//time_window_in_sec)\n",
    "        for i in range(0, number_of_window):\n",
    "            windows += [(i*time_window_in_sec, (i+1)*time_window_in_sec)]\n",
    "        windows += [((i+1)*time_window_in_sec, max(target_timestamps))]\n",
    "        # [(0, 5], (5, 10], ...]\n",
    "\n",
    "        # acoustic features process\n",
    "        a_file = os.path.join(data_dir, modality_dir_map[\"acoustic\"], f\"{video_id}_acousticFeatures.csv\")\n",
    "        a_df = pd.read_csv(a_file)\n",
    "        a_features = np.array(preprocess[\"acoustic\"](a_df))\n",
    "        a_timestamps = np.array(preprocess[\"acoustic_timer\"](a_df))\n",
    "        a_feature_dim = a_features.shape[1]\n",
    "        assert a_features.shape[0] == a_timestamps.shape[0]\n",
    "        sampled_a_features_raw = [[] for i in range(len(windows))]\n",
    "        for i in range(0, a_timestamps.shape[0]):\n",
    "            # using mod to hash to the correct bucket.\n",
    "            hash_in_window = int(a_timestamps[i]//time_window_in_sec)\n",
    "            if hash_in_window >= len(windows):\n",
    "                continue # we cannot predict after ratings max.\n",
    "            sampled_a_features_raw[hash_in_window].append(a_features[i])\n",
    "        sampled_a_features = []\n",
    "        for window in sampled_a_features_raw:\n",
    "            # only acoustic need to consider this I think.\n",
    "            if len(window) == 0:\n",
    "                collate_window = np.zeros(a_feature_dim)\n",
    "            else:\n",
    "                collate_window = np.mean(np.array(window), axis=0)\n",
    "            sampled_a_features.append(collate_window)\n",
    "        \n",
    "        \n",
    "        # linguistic features process\n",
    "        l_file = os.path.join(data_dir, modality_dir_map[\"linguistic\"], f\"{video_id}_aligned.tsv\")\n",
    "        l_df = pd.read_csv(l_file, sep='\\t')\n",
    "        l_words = np.array(preprocess[\"linguistic\"](l_df))\n",
    "        l_words = [w.strip().lower() for w in l_words]\n",
    "        l_timestamps = np.array(preprocess[\"linguistic_timer\"](l_df))\n",
    "        assert len(l_words) == l_timestamps.shape[0]\n",
    "        sampled_l_features_raw = [[] for i in range(len(windows))]\n",
    "        for i in range(0, l_timestamps.shape[0]):\n",
    "            # using mod to hash to the correct bucket.\n",
    "            hash_in_window = int(l_timestamps[i]//time_window_in_sec)\n",
    "            if hash_in_window >= len(windows):\n",
    "                continue # we cannot predict after ratings max.\n",
    "            sampled_l_features_raw[hash_in_window].append(l_words[i])\n",
    "        sampled_l_features = []\n",
    "        sampled_l_window_length = []\n",
    "        for window in sampled_l_features_raw:\n",
    "            complete_window_word = [\"[CLS]\"] + window + [\"[SEP]\"]\n",
    "            token_ids = linguistic_tokenizer.convert_tokens_to_ids(complete_window_word)\n",
    "            sampled_l_window_length += [len(token_ids)]\n",
    "            for _ in range(0, max_window_l_length-len(token_ids)):\n",
    "                token_ids.append(linguistic_tokenizer.pad_token_id)\n",
    "            sampled_l_features += [token_ids]\n",
    "\n",
    "        \n",
    "        # visual features process\n",
    "        # for visual, we actually need to active control what image we load, we\n",
    "        # cannot just load all images, it will below memory.\n",
    "        fps=30 # We may need to dynamically figure out this number?\n",
    "        frame_names = []\n",
    "        for f in listdir(os.path.join(data_dir, modality_dir_map[\"visual\"], video_id)):\n",
    "            if \".jpg\" in f:\n",
    "                frame_names += [(int(f.split(\"_\")[0][5:])*(1.0/fps), f)]\n",
    "        frame_names.sort(key=lambda x:x[0])\n",
    "        sampled_v_features_raw = [[] for i in range(len(windows))]\n",
    "        for f in frame_names:\n",
    "            # using mod to hash to the correct bucket.\n",
    "            hash_in_window = int(f[0]//time_window_in_sec)\n",
    "            if hash_in_window >= len(windows):\n",
    "                continue # we cannot predict after ratings max.\n",
    "            sampled_v_features_raw[hash_in_window].append(f)\n",
    "\n",
    "        sampled_v_features = []\n",
    "        for window in sampled_v_features_raw:\n",
    "            if len(window) == 0:\n",
    "                f_data = np.zeros((224,224,3))\n",
    "            else:\n",
    "                # we collate by using the last frame in the time window.\n",
    "                f = window[-1]\n",
    "                f_path = os.path.join(data_dir, modality_dir_map[\"visual\"], video_id, f[1])\n",
    "                f_image = Image.open(f_path)\n",
    "                f_data = asarray(f_image)\n",
    "            sampled_v_features.append(f_data)\n",
    "\n",
    "        # ratings (target)\n",
    "        target_id = video_id.split(\"_\")[0][2:] + \"_\" + video_id.split(\"_\")[1][3:]\n",
    "        target_file = os.path.join(target_data_dir, modality_dir_map[\"target\"], f\"target_{target_id}_normal.csv\")\n",
    "        target_df = pd.read_csv(target_file)\n",
    "        target_ratings = np.array(preprocess[\"target\"](target_df))\n",
    "        target_timestamps = np.array(preprocess[\"target_timer\"](target_df))\n",
    "        assert target_ratings.shape[0] == target_timestamps.shape[0]\n",
    "        sampled_ratings_raw = [[] for i in range(len(windows))]\n",
    "        for i in range(0, target_timestamps.shape[0]):\n",
    "            # using mod to hash to the correct bucket.\n",
    "            hash_in_window = int(target_timestamps[i]//time_window_in_sec)\n",
    "            sampled_ratings_raw[hash_in_window].append(target_ratings[i])\n",
    "        sampled_ratings = []\n",
    "        for window in sampled_ratings_raw:\n",
    "            collate_window = np.mean(np.array(window), axis=0)\n",
    "            sampled_ratings.append(collate_window)\n",
    "        \n",
    "        # we truncate features based on linguistic avaliabilities.\n",
    "        assert len(sampled_a_features) == len(sampled_l_features)\n",
    "        assert len(sampled_a_features) == len(sampled_v_features)\n",
    "        \n",
    "        max_window_cutoff_l = int(max(l_timestamps)//time_window_in_sec)\n",
    "        max_window_cutoff_a = int(max(a_timestamps)//time_window_in_sec)\n",
    "        max_window_cutoff_v = int(frame_names[-1][0]//time_window_in_sec)\n",
    "        max_window_cutoff = min([max_window_cutoff_l, max_window_cutoff_a, max_window_cutoff_v])\n",
    "        sampled_a_features = sampled_a_features[:max_window_cutoff]\n",
    "        sampled_l_features = sampled_l_features[:max_window_cutoff]\n",
    "        sampled_v_features = sampled_v_features[:max_window_cutoff]\n",
    "        sampled_ratings = sampled_ratings[:max_window_cutoff]\n",
    "        sampled_l_window_length = sampled_l_window_length[:max_window_cutoff]\n",
    "        \n",
    "        video_struct = {\n",
    "            \"video_id\": video_id,\n",
    "            \"a_feature\": sampled_a_features,\n",
    "            \"l_feature\": sampled_l_features,\n",
    "            \"v_feature\": sampled_v_features,\n",
    "            \"rating\": sampled_ratings,\n",
    "            \"l_inwindow_length\": sampled_l_window_length,\n",
    "            \"seq_len\": len(sampled_a_features)\n",
    "        }\n",
    "        video_count += 1\n",
    "        SEND_videos += [video_struct]\n",
    "        if len(sampled_a_features) > max_seq_len:\n",
    "            max_seq_len = len(sampled_a_features)\n",
    "    \n",
    "    # padding based on length\n",
    "    for video_struct in SEND_videos:\n",
    "        for i in range(max_seq_len-video_struct[\"seq_len\"]):\n",
    "            video_struct[\"a_feature\"].append(np.zeros(a_feature_dim))\n",
    "            video_struct[\"l_feature\"].append(np.zeros(max_window_l_length))\n",
    "            video_struct[\"v_feature\"].append(np.zeros((224,224,3)))\n",
    "            video_struct[\"rating\"].append(0.0)\n",
    "            video_struct[\"l_inwindow_length\"].append(0)\n",
    "        video_struct[\"a_feature\"] = torch.tensor(video_struct[\"a_feature\"])\n",
    "        video_struct[\"l_feature\"] = torch.LongTensor(video_struct[\"l_feature\"])\n",
    "        video_struct[\"v_feature\"] = torch.tensor(video_struct[\"v_feature\"])\n",
    "        video_struct[\"rating\"] = torch.tensor(video_struct[\"rating\"])\n",
    "        \n",
    "    return SEND_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arg_parse():\n",
    "    \n",
    "    # This is a single loop to generate the dataset.\n",
    "    n_processes = 1\n",
    "    mode = \"all\"\n",
    "    n_command_struct = 10000\n",
    "    grid_size = 6\n",
    "    n_object_max = 10\n",
    "    seed = 42\n",
    "    date = \"2021-05-07\"\n",
    "    per_command_world_retry_max = 200\n",
    "    per_command_world_target_count = 10 # for each command, we target to have 50 shapeWorld!\n",
    "    resumed_from_file_path = \"\"\n",
    "    is_tensorboard = False\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='multimodal emotion analysis argparse.')\n",
    "    # Experiment management:\n",
    "\n",
    "    parser.add_argument('--train_batch_size', type=int, default=6,\n",
    "                        help='Training batch size.')\n",
    "    parser.add_argument('--eval_batch_size', type=int, default=12,\n",
    "                        help='Evaluation batch size.')\n",
    "    parser.add_argument('--lr', type=float, default=1e-4,\n",
    "                        help='Learning rate.')\n",
    "    parser.add_argument('--seed', type=int, default=42,\n",
    "                        help='Random seed.')\n",
    "    \n",
    "    parser.add_argument('--resumed_from_file_path', type=str, default=\"\",\n",
    "                        help='Whether to resume for this file.')\n",
    "    parser.add_argument('--data_dir', type=str, default=\"../../SENDv1-data/\",\n",
    "                        help='Whether to resume for this file.')\n",
    "    parser.add_argument('--output_dir', type=str, default=\"../default_output_log/\",\n",
    "                        help='Whether to resume for this file.')\n",
    "    parser.add_argument(\"--is_tensorboard\",\n",
    "                        default=False,\n",
    "                        action='store_true',\n",
    "                        help=\"Whether to use tensorboard.\")\n",
    "    parser.add_argument(\"--eval_only\",\n",
    "                        default=False,\n",
    "                        action='store_true',\n",
    "                        help=\"Whether to use tensorboard.\")\n",
    "    \n",
    "    parser.set_defaults(\n",
    "        # Exp management:\n",
    "        seed=42,\n",
    "    )\n",
    "    try:\n",
    "        get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "        args = parser.parse_args([])\n",
    "    except:\n",
    "        args = parser.parse_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model with the following parameters: \n",
      "Namespace(data_dir='../../SENDv1-data/', eval_batch_size=12, eval_only=False, is_tensorboard=True, lr=0.0001, output_dir='../default_output_log/', resumed_from_file_path='', seed=42, train_batch_size=6)\n",
      "Finish Loading Datasets...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Loading arguments\n",
    "    args = arg_parse()\n",
    "    try:        \n",
    "        get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "        # Experiment management:\n",
    "        args.train_batch_size=6\n",
    "        args.eval_batch_size=12\n",
    "        args.lr=1e-4\n",
    "        args.seed=42\n",
    "        args.is_tensorboard=True # Let us try this!\n",
    "        args.output_dir=\"../default_output_log/\"\n",
    "        is_jupyter = True\n",
    "\n",
    "    except:\n",
    "        is_jupyter = False\n",
    "        \n",
    "    # Create output directory if not exists.\n",
    "    pathlib.Path(args.output_dir).mkdir(parents=True, exist_ok=True) \n",
    "    \n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO, \n",
    "        format='%(asctime)s %(levelname)-8s %(message)s', \n",
    "        datefmt='%a, %d %b %Y %H:%M:%S', \n",
    "        filename=os.path.join(args.output_dir, \"training.log\"),\n",
    "    )\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.getLogger().addHandler(logging.StreamHandler(os.sys.stdout))\n",
    "    \n",
    "    logger.info(\"Training the model with the following parameters: \")\n",
    "    logger.info(args)\n",
    "    \n",
    "    # We don't allow flexibility here..\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"bert-base-uncased\",\n",
    "        use_fast=False,\n",
    "        cache_dir=\"../.huggingface_cache/\"\n",
    "    )\n",
    "    \n",
    "    train_SEND_features = None\n",
    "    test_SEND_features = None\n",
    "    if args.eval_only:\n",
    "        # Training data loading \n",
    "        train_modalities_data_dir = os.path.join(args.data_dir, \"features/Train/\")\n",
    "        train_target_data_dir = os.path.join(args.data_dir, \"ratings/Train\")\n",
    "\n",
    "        test_modalities_data_dir = os.path.join(args.data_dir, \"features/Valid/\")\n",
    "        test_target_data_dir = os.path.join(args.data_dir, \"ratings/Valid\")\n",
    "        \n",
    "        train_SEND_features = preprocess_SEND_files(\n",
    "            train_modalities_data_dir,\n",
    "            train_target_data_dir,\n",
    "            linguistic_tokenizer=tokenizer,\n",
    "        )\n",
    "        test_SEND_features = preprocess_SEND_files(\n",
    "            test_modalities_data_dir,\n",
    "            test_target_data_dir,\n",
    "            linguistic_tokenizer=tokenizer,\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        test_modalities_data_dir = os.path.join(args.data_dir, \"features/Test/\")\n",
    "        test_target_data_dir = os.path.join(args.data_dir, \"ratings/Test\")\n",
    "    \n",
    "        test_SEND_features = preprocess_SEND_files(\n",
    "            test_modalities_data_dir,\n",
    "            test_target_data_dir,\n",
    "            linguistic_tokenizer=tokenizer,\n",
    "        )\n",
    "    logger.info(\"Finish Loading Datasets...\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_a_feature = torch.stack([video_struct[\"a_feature\"] for video_struct in test_SEND_features])\n",
    "test_input_l_feature = torch.stack([video_struct[\"l_feature\"] for video_struct in test_SEND_features])\n",
    "test_input_v_feature = torch.stack([video_struct[\"v_feature\"] for video_struct in test_SEND_features])\n",
    "test_rating_labels = torch.stack([video_struct[\"rating\"] for video_struct in test_SEND_features])\n",
    "test_l_seq_lens = torch.tensor([video_struct[\"l_inwindow_length\"] for video_struct in test_SEND_features], dtype=torch.long)\n",
    "test_seq_lens = torch.tensor([[video_struct[\"seq_len\"]] for video_struct in test_SEND_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building up our SEND model.\n",
    "from models.BERT import *\n",
    "from models.VGGFace2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalEmotionPrediction(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        linguistic_model=\"bert-base-uncased\",\n",
    "        visual_model=\"vggface-2\",\n",
    "        visual_model_path=\"../saved-models/resnet50_scratch_dag.pth\",\n",
    "        acoustic_model=\"mlp\",\n",
    "        cache_dir=\"../.huggingface_cache/\",\n",
    "    ):\n",
    "        super(MultimodalEmotionPrediction, self).__init__()\n",
    "        \n",
    "        # Loading BERT using huggingface?\n",
    "        linguistic_config = AutoConfig.from_pretrained(\n",
    "            linguistic_model,\n",
    "            cache_dir=cache_dir\n",
    "        )\n",
    "        self.linguistic_encoder = LinguisticEncoderBERT.from_pretrained(\n",
    "            linguistic_model,\n",
    "            from_tf=False,\n",
    "            config=linguistic_config,\n",
    "            cache_dir=cache_dir\n",
    "        )\n",
    "        \n",
    "        # Loading visual model using vggface-2\n",
    "        self.visual_encoder = Resnet50_scratch_dag()\n",
    "        state_dict = torch.load(visual_model_path)\n",
    "        self.visual_encoder.load_state_dict(state_dict)\n",
    "        \n",
    "        # Creating acoustic model.\n",
    "        acoustic_dim = 88\n",
    "        self.acoustic_encoder = nn.Linear(acoustic_dim, 128)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing LinguisticEncoderBERT: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.embeddings.word_embeddings.weight']\n",
      "- This IS expected if you are initializing LinguisticEncoderBERT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LinguisticEncoderBERT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LinguisticEncoderBERT were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.pretrain_word_embeddings.weight', 'bert.embeddings.study_abroad_transformation_layer.weight', 'bert.embeddings.study_abroad_transformation_layer.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = MultimodalEmotionPrediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinguisticEncoderBERT(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (pretrain_word_embeddings): Embedding(31160, 300, padding_idx=0)\n",
       "      (study_abroad_transformation_layer): Linear(in_features=300, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linguistic_encoder(\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
