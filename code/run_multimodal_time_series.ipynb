{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "\n",
    "import argparse\n",
    "from collections import namedtuple, OrderedDict\n",
    "import itertools\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "from typing import List\n",
    "from typing import Dict\n",
    "import random\n",
    "from itertools import product\n",
    "import copy\n",
    "import re\n",
    "import random\n",
    "import hashlib\n",
    "import pathlib\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "from scipy.stats import pearsonr\n",
    "import wandb\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "\n",
    "import logging\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "# Building up our SEND model.\n",
    "from models.BERT import *\n",
    "from models.VGGFace2 import *\n",
    "from models.optimization import *\n",
    "\n",
    "class InputFeature:\n",
    "    \n",
    "    def __init__(\n",
    "        self, video_id=\"\",\n",
    "        acoustic_feature=[],\n",
    "        linguistic_feature=[],\n",
    "        visual_feature=[],\n",
    "        labels=[],\n",
    "    ):\n",
    "        self.video_id = video_id\n",
    "        self.acoustic_feature = acoustic_feature\n",
    "        self.linguistic_feature = linguistic_feature\n",
    "        self.visual_feature = visual_feature\n",
    "        self.labels = labels\n",
    "        \n",
    "def preprocess_SEND_files(\n",
    "    data_dir, # Multitmodal X\n",
    "    target_data_dir, # Y\n",
    "    use_target_ratings,\n",
    "    time_window_in_sec=4.0,\n",
    "    modality_dir_map = {\"acoustic\": \"acoustic-egemaps\",  \n",
    "                        \"linguistic\": \"linguistic-word-level\", # we don't load features\n",
    "                        \"visual\": \"image-raw\", # image is nested,\n",
    "                        \"target\": \"observer_EWE\",\n",
    "                       },\n",
    "    preprocess= {'acoustic': lambda df : df.loc[:,' F0semitoneFrom27.5Hz_sma3nz_amean':' equivalentSoundLevel_dBp'],\n",
    "                 'acoustic_timer': lambda df : df.loc[:,' frameTime'],\n",
    "                 'linguistic': lambda df : df.loc[:,'word'],\n",
    "                 'linguistic_timer': lambda df : df.loc[:,'time-offset'],\n",
    "                 'target': lambda df : ((df.loc[:,'evaluatorWeightedEstimate'] / 50.0) - 1.0),\n",
    "                 'target_timer': lambda df : df.loc[:,'time'],\n",
    "                },\n",
    "    linguistic_tokenizer=None,\n",
    "    pad_symbol=0,\n",
    "    max_number_of_file=-1\n",
    "):\n",
    "    \n",
    "    import time\n",
    "\n",
    "    start = time.time()    \n",
    "    SEND_videos = []\n",
    "    \n",
    "    # basically, let us gett all the video ids?\n",
    "    a_ids = [f.split(\"_\")[0]+\"_\"+f.split(\"_\")[1] \n",
    "             for f in listdir(os.path.join(data_dir, modality_dir_map[\"acoustic\"])) \n",
    "             if isfile(os.path.join(data_dir, modality_dir_map[\"acoustic\"], f))]\n",
    "    l_ids = [f.split(\"_\")[0]+\"_\"+f.split(\"_\")[1] \n",
    "             for f in listdir(os.path.join(data_dir, modality_dir_map[\"linguistic\"])) \n",
    "             if isfile(os.path.join(data_dir, modality_dir_map[\"linguistic\"], f))]\n",
    "    v_ids = [f.split(\"_\")[0]+\"_\"+f.split(\"_\")[1] \n",
    "             for f in listdir(os.path.join(data_dir, modality_dir_map[\"visual\"])) \n",
    "             if f != \".DS_Store\"]\n",
    "    assert len(a_ids) == len(l_ids) and len(l_ids) == len(v_ids)\n",
    "    assert len(set(a_ids).intersection(set(l_ids))) == len(l_ids)\n",
    "    assert len(set(a_ids).intersection(set(v_ids))) == len(v_ids)\n",
    "    \n",
    "    # We need the first pass for linguistic modality process?\n",
    "    max_window_l_length = -1\n",
    "    for video_id in a_ids: # pick any one!\n",
    "        # linguistic features process\n",
    "        l_file = os.path.join(data_dir, modality_dir_map[\"linguistic\"], f\"{video_id}_aligned.tsv\")\n",
    "        l_df = pd.read_csv(l_file, sep='\\t')\n",
    "        #l_words = np.array(preprocess[\"linguistic\"](l_df))\n",
    "        #l_words = [w.strip().lower() for w in l_words]\n",
    "        l_words = []\n",
    "        l_timestamps = []\n",
    "        head = True\n",
    "        with open(l_file) as fp:\n",
    "            for line in fp:\n",
    "                if head:\n",
    "                    head = False\n",
    "                    continue\n",
    "                l_words.append(line.strip().split(\"\\t\")[2].lower().strip())\n",
    "                l_timestamps.append(float(line.strip().split(\"\\t\")[1]))\n",
    "\n",
    "        #l_timestamps = np.array(preprocess[\"linguistic_timer\"](l_df))\n",
    "        l_timestamps = np.array(l_timestamps)\n",
    "        # sample based on interval\n",
    "        current_time = 0.0\n",
    "        keep_first = True\n",
    "        sampled_l_words = [] # different from other modality, it is essentially a list of list!\n",
    "        tmp_words = []\n",
    "        for i in range(0, l_timestamps.shape[0]):\n",
    "            if keep_first:\n",
    "                sampled_l_words += [[]]\n",
    "                keep_first = False\n",
    "            if l_timestamps[i] >= current_time+time_window_in_sec:\n",
    "                sampled_l_words.append(tmp_words)\n",
    "                tmp_words = [l_words[i]] # reinit the buffer\n",
    "                current_time += time_window_in_sec\n",
    "                continue\n",
    "            tmp_words += [l_words[i]]\n",
    "        # overflow\n",
    "        if len(tmp_words) > 0:\n",
    "            sampled_l_words.append(tmp_words)\n",
    "        for window_words in sampled_l_words:\n",
    "            window_str = \" \".join(window_words)\n",
    "            window_tokens = linguistic_tokenizer.tokenize(window_str)\n",
    "            token_ids = linguistic_tokenizer.convert_tokens_to_ids(window_tokens)\n",
    "            if len(token_ids) > max_window_l_length:\n",
    "                max_window_l_length = len(token_ids)\n",
    "\n",
    "    max_window_l_length += 2 # the start and the end token\n",
    "    \n",
    "    if max_number_of_file != -1:\n",
    "        logger.info(f\"WARNING: Only loading #{max_number_of_file} videos.\")\n",
    "    max_seq_len = -1\n",
    "    video_count = 0\n",
    "    for video_id in a_ids: # pick any one!\n",
    "        if max_number_of_file != -1 and video_count >= max_number_of_file:\n",
    "            break # we enforce!\n",
    "        if video_count > 1 and video_count%100 == 0:\n",
    "            logger.info(f\"Processed #{len(SEND_videos)} videos.\")\n",
    "            # logger.info(SEND_videos[-1])\n",
    "        \n",
    "        # we need to fix this to get features aligned.\n",
    "        \n",
    "        # Step 1: Load rating data, and we can get window partitioned according to our interval.\n",
    "        target_id = video_id.split(\"_\")[0][2:] + \"_\" + video_id.split(\"_\")[1][3:]\n",
    "        if use_target_ratings:\n",
    "            target_file = os.path.join(target_data_dir, modality_dir_map[\"target\"], f\"target_{target_id}_normal.csv\")\n",
    "        else:\n",
    "            target_file = os.path.join(target_data_dir, modality_dir_map[\"target\"], f\"results_{target_id}.csv\")\n",
    "        target_df = pd.read_csv(target_file)\n",
    "        target_ratings = np.array(preprocess[\"target\"](target_df))\n",
    "        target_timestamps = np.array(preprocess[\"target_timer\"](target_df))\n",
    "        assert target_ratings.shape[0] == target_timestamps.shape[0]\n",
    "        windows = []\n",
    "        number_of_window = int(max(target_timestamps)//time_window_in_sec)\n",
    "        for i in range(0, number_of_window):\n",
    "            windows += [(i*time_window_in_sec, (i+1)*time_window_in_sec)]\n",
    "        windows += [((i+1)*time_window_in_sec, max(target_timestamps))]\n",
    "        # [(0, 5], (5, 10], ...]\n",
    "\n",
    "        # acoustic features process\n",
    "        a_file = os.path.join(data_dir, modality_dir_map[\"acoustic\"], f\"{video_id}_acousticFeatures.csv\")\n",
    "        a_df = pd.read_csv(a_file)\n",
    "        a_features = np.array(preprocess[\"acoustic\"](a_df))\n",
    "        a_timestamps = np.array(preprocess[\"acoustic_timer\"](a_df))\n",
    "        a_feature_dim = a_features.shape[1]\n",
    "        assert a_features.shape[0] == a_timestamps.shape[0]\n",
    "        sampled_a_features_raw = [[] for i in range(len(windows))]\n",
    "        for i in range(0, a_timestamps.shape[0]):\n",
    "            # using mod to hash to the correct bucket.\n",
    "            hash_in_window = int(a_timestamps[i]//time_window_in_sec)\n",
    "            if hash_in_window >= len(windows):\n",
    "                continue # we cannot predict after ratings max.\n",
    "            sampled_a_features_raw[hash_in_window].append(a_features[i])\n",
    "        sampled_a_features = []\n",
    "        for window in sampled_a_features_raw:\n",
    "            # only acoustic need to consider this I think.\n",
    "            if len(window) == 0:\n",
    "                collate_window = np.zeros(a_feature_dim)\n",
    "            else:\n",
    "                collate_window = np.mean(np.array(window), axis=0)\n",
    "            sampled_a_features.append(collate_window)\n",
    "        \n",
    "        \n",
    "        # linguistic features process\n",
    "        l_file = os.path.join(data_dir, modality_dir_map[\"linguistic\"], f\"{video_id}_aligned.tsv\")\n",
    "        l_df = pd.read_csv(l_file, sep='\\t')\n",
    "        # the following line is buggy, it may parse file incorrectly!\n",
    "        #l_words = np.array(preprocess[\"linguistic\"](l_df))\n",
    "        #l_words = [w.strip().lower() for w in l_words]\n",
    "        l_words = []\n",
    "        l_timestamps = []\n",
    "        head = True\n",
    "        with open(l_file) as fp:\n",
    "            for line in fp:\n",
    "                if head:\n",
    "                    head = False\n",
    "                    continue\n",
    "                l_words.append(line.strip().split(\"\\t\")[2].lower().strip())\n",
    "                l_timestamps.append(float(line.strip().split(\"\\t\")[1]))\n",
    "        #l_timestamps = np.array(preprocess[\"linguistic_timer\"](l_df))\n",
    "        l_timestamps = np.array(l_timestamps)\n",
    "        assert len(l_words) == l_timestamps.shape[0]\n",
    "        \n",
    "        sampled_l_features_raw = [[] for i in range(len(windows))]\n",
    "        for i in range(0, l_timestamps.shape[0]):\n",
    "            # using mod to hash to the correct bucket.\n",
    "            hash_in_window = int(l_timestamps[i]//time_window_in_sec)\n",
    "            if hash_in_window >= len(windows):\n",
    "                continue # we cannot predict after ratings max.\n",
    "            sampled_l_features_raw[hash_in_window].append(l_words[i])\n",
    "\n",
    "        sampled_l_features = []\n",
    "        sampled_l_mask = []\n",
    "        sampled_l_segment_ids = []\n",
    "        for window in sampled_l_features_raw:\n",
    "            window_str = \" \".join(window)\n",
    "            window = linguistic_tokenizer.tokenize(window_str)\n",
    "            complete_window_word = [\"[CLS]\"] + window + [\"[SEP]\"]\n",
    "            token_ids = linguistic_tokenizer.convert_tokens_to_ids(complete_window_word)\n",
    "            input_mask = [1 for _ in range(len(token_ids))]\n",
    "            for _ in range(0, max_window_l_length-len(token_ids)):\n",
    "                token_ids.append(linguistic_tokenizer.pad_token_id)\n",
    "                input_mask.append(0)\n",
    "            segment_ids = [0] * len(token_ids)\n",
    "            sampled_l_features += [token_ids]\n",
    "            sampled_l_mask += [input_mask]\n",
    "            sampled_l_segment_ids += [segment_ids]\n",
    "\n",
    "\n",
    "        # visual features process\n",
    "        # for visual, we actually need to active control what image we load, we\n",
    "        # cannot just load all images, it will below memory.\n",
    "        fps=30 # We may need to dynamically figure out this number?\n",
    "        frame_names = []\n",
    "        for f in listdir(os.path.join(data_dir, modality_dir_map[\"visual\"], video_id)):\n",
    "            if \".jpg\" in f:\n",
    "                frame_names += [(int(f.split(\"_\")[0][5:])*(1.0/fps), f)]\n",
    "        frame_names.sort(key=lambda x:x[0])\n",
    "        sampled_v_features_raw = [[] for i in range(len(windows))]\n",
    "        for f in frame_names:\n",
    "            # using mod to hash to the correct bucket.\n",
    "            hash_in_window = int(f[0]//time_window_in_sec)\n",
    "            if hash_in_window >= len(windows):\n",
    "                continue # we cannot predict after ratings max.\n",
    "            sampled_v_features_raw[hash_in_window].append(f)\n",
    "            \n",
    "        sampled_v_features = []\n",
    "        for window in sampled_v_features_raw:\n",
    "            if len(window) == 0:\n",
    "                f_data = np.zeros((224,224,3))\n",
    "            else:\n",
    "                # we collate by using the last frame in the time window.\n",
    "                f = window[-1]\n",
    "                f_path = os.path.join(data_dir, modality_dir_map[\"visual\"], video_id, f[1])\n",
    "                f_image = Image.open(f_path)\n",
    "                f_data = asarray(f_image)\n",
    "                f_data = f_data[...,::-1] # reverse the order.\n",
    "            sampled_v_features.append(f_data)\n",
    "\n",
    "        # ratings (target)\n",
    "        target_id = video_id.split(\"_\")[0][2:] + \"_\" + video_id.split(\"_\")[1][3:]\n",
    "        if use_target_ratings:\n",
    "            target_file = os.path.join(target_data_dir, modality_dir_map[\"target\"], f\"target_{target_id}_normal.csv\")\n",
    "        else:\n",
    "            target_file = os.path.join(target_data_dir, modality_dir_map[\"target\"], f\"results_{target_id}.csv\")\n",
    "        target_df = pd.read_csv(target_file)\n",
    "        target_ratings = np.array(preprocess[\"target\"](target_df))\n",
    "        target_timestamps = np.array(preprocess[\"target_timer\"](target_df))\n",
    "        assert target_ratings.shape[0] == target_timestamps.shape[0]\n",
    "        sampled_ratings_raw = [[] for i in range(len(windows))]\n",
    "        for i in range(0, target_timestamps.shape[0]):\n",
    "            # using mod to hash to the correct bucket.\n",
    "            hash_in_window = int(target_timestamps[i]//time_window_in_sec)\n",
    "            sampled_ratings_raw[hash_in_window].append(target_ratings[i])\n",
    "        sampled_ratings = []\n",
    "        for window in sampled_ratings_raw:\n",
    "            collate_window = np.mean(np.array(window), axis=0)\n",
    "            sampled_ratings.append(collate_window)\n",
    "        \n",
    "        # we truncate features based on linguistic avaliabilities.\n",
    "        assert len(sampled_a_features) == len(sampled_l_features)\n",
    "        assert len(sampled_a_features) == len(sampled_v_features)\n",
    "        \n",
    "        max_window_cutoff_l = int(max(l_timestamps)//time_window_in_sec)\n",
    "        max_window_cutoff_a = int(max(a_timestamps)//time_window_in_sec)\n",
    "        max_window_cutoff_v = int(frame_names[-1][0]//time_window_in_sec)\n",
    "        max_window_cutoff = min([max_window_cutoff_l, max_window_cutoff_a, max_window_cutoff_v])\n",
    "        sampled_a_features = sampled_a_features[:max_window_cutoff]\n",
    "        sampled_l_features = sampled_l_features[:max_window_cutoff]\n",
    "        sampled_v_features = sampled_v_features[:max_window_cutoff]\n",
    "        sampled_ratings = sampled_ratings[:max_window_cutoff]\n",
    "        sampled_l_mask = sampled_l_mask[:max_window_cutoff]\n",
    "        sampled_l_segment_ids = sampled_l_segment_ids[:max_window_cutoff]\n",
    "        input_mask = np.ones(len(sampled_a_features)).tolist()\n",
    "        max_seq_len = 60\n",
    "        seq_len = len(sampled_a_features)\n",
    "        for i in range(max_seq_len-len(sampled_a_features)):\n",
    "            sampled_a_features.append(np.zeros(a_feature_dim))\n",
    "            sampled_l_features.append(np.zeros(max_window_l_length))\n",
    "            sampled_l_mask.append(np.zeros(max_window_l_length))\n",
    "            sampled_l_segment_ids.append(np.zeros(max_window_l_length))\n",
    "            sampled_v_features.append(np.zeros((224,224,3)))\n",
    "            sampled_ratings.append(0.0)\n",
    "            input_mask.append(0)\n",
    "\n",
    "        sampled_a_features = torch.tensor(sampled_a_features)\n",
    "        sampled_l_features = torch.LongTensor(sampled_l_features)\n",
    "        sampled_l_mask = torch.LongTensor(sampled_l_mask)\n",
    "        sampled_l_segment_ids = torch.LongTensor(sampled_l_segment_ids)\n",
    "        processed_tensor = torch.tensor(sampled_v_features).float()\n",
    "        processed_tensor[..., 0] -= 91.4953\n",
    "        processed_tensor[..., 1] -= 103.8827\n",
    "        processed_tensor[..., 2] -= 131.0912\n",
    "        sampled_v_features = processed_tensor\n",
    "        sampled_ratings = torch.tensor(sampled_ratings)\n",
    "        input_mask = torch.LongTensor(input_mask)\n",
    "        \n",
    "        video_struct = {\n",
    "            \"video_id\": video_id,\n",
    "            \"a_feature\": sampled_a_features,\n",
    "            \"l_feature\": sampled_l_features,\n",
    "            \"l_mask\": sampled_l_mask,\n",
    "            \"l_segment_ids\": sampled_l_segment_ids,\n",
    "            \"v_feature\": sampled_v_features,\n",
    "            \"rating\": sampled_ratings,\n",
    "            \"seq_len\": seq_len,\n",
    "            \"input_mask\": input_mask\n",
    "        }\n",
    "        video_count += 1\n",
    "        SEND_videos += [video_struct]\n",
    "    \n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    logger.info(f\"Time elapsed for first-pass: {elapsed}\")\n",
    "        \n",
    "    return SEND_videos\n",
    "\n",
    "def eval_ccc(y_true, y_pred):\n",
    "    \"\"\"Computes concordance correlation coefficient.\"\"\"\n",
    "    true_mean = np.mean(y_true)\n",
    "    true_var = np.var(y_true)\n",
    "    pred_mean = np.mean(y_pred)\n",
    "    pred_var = np.var(y_pred)\n",
    "    covar = np.cov(y_true, y_pred, bias=True)[0][1]\n",
    "    ccc = 2*covar / (true_var + pred_var +  (pred_mean-true_mean) ** 2)\n",
    "    return ccc\n",
    "\n",
    "class MultimodalEmotionPrediction(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        linguistic_model=\"bert-base-uncased\",\n",
    "        visual_model=\"vggface-2\",\n",
    "        visual_model_path=\"../saved-models/resnet50_scratch_dag.pth\",\n",
    "        acoustic_model=\"mlp\",\n",
    "        cache_dir=\"../.huggingface_cache/\",\n",
    "    ):\n",
    "        super(MultimodalEmotionPrediction, self).__init__()\n",
    "        \n",
    "        # Loading BERT using huggingface?\n",
    "        linguistic_config = AutoConfig.from_pretrained(\n",
    "            linguistic_model,\n",
    "            cache_dir=cache_dir\n",
    "        )\n",
    "        self.linguistic_encoder = LinguisticEncoderBERT.from_pretrained(\n",
    "            linguistic_model,\n",
    "            from_tf=False,\n",
    "            config=linguistic_config,\n",
    "            cache_dir=cache_dir\n",
    "        )\n",
    "        # let us disenable gradient prop\n",
    "        # for name, param in self.linguistic_encoder.named_parameters():\n",
    "        #     param.requires_grad = False\n",
    "        \n",
    "        # Loading visual model using vggface-2\n",
    "        self.visual_encoder = Resnet50_scratch_dag()\n",
    "        state_dict = torch.load(visual_model_path)\n",
    "        self.visual_encoder.load_state_dict(state_dict)\n",
    "        self.visual_reducer = nn.Linear(2048, 768)\n",
    "\n",
    "        # Rating lstm.\n",
    "        # hidden_dim = 128\n",
    "        hidden_dim = 768        \n",
    "        self.rating_decoder = nn.LSTM(\n",
    "                                hidden_dim, 64, 1, \n",
    "                                batch_first=True, bidirectional=False)\n",
    "                                              \n",
    "        # Rating decoder.\n",
    "        self.rating_output = nn.Sequential(\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "        self.acoustic_encoder = nn.Linear(88, 32)\n",
    "        self.rating_decoder_a = nn.LSTM(\n",
    "                                32, 1, 1, \n",
    "                                batch_first=True, bidirectional=False)\n",
    "            \n",
    "        self.rating_decoder_v = nn.LSTM(\n",
    "                                768, 1, 1, \n",
    "                                batch_first=True, bidirectional=False)\n",
    "            \n",
    "    def forward(\n",
    "        self, input_a_feature, input_l_feature, \n",
    "        input_l_mask, input_l_segment_ids, \n",
    "        input_v_feature, train_rating_labels, input_mask,\n",
    "    ):\n",
    "        \n",
    "        # linguistic encoder\n",
    "        batch_size, seq_len = input_l_feature.shape[0], input_l_feature.shape[1]\n",
    "        input_l_feature = input_l_feature.reshape(batch_size*seq_len, -1)\n",
    "        input_l_mask = input_l_mask.reshape(batch_size*seq_len, -1)\n",
    "        input_l_segment_ids = input_l_segment_ids.reshape(batch_size*seq_len, -1)\n",
    "        \n",
    "        l_decode = self.linguistic_encoder(\n",
    "            input_ids=input_l_feature,\n",
    "            attention_mask=input_l_mask,\n",
    "            token_type_ids=input_l_segment_ids,\n",
    "        )\n",
    "        l_decode = l_decode.reshape(batch_size, seq_len, -1)\n",
    "        \n",
    "        # visual encoder\n",
    "        input_v_feature = input_v_feature.reshape(batch_size*seq_len, 224, 224, 3)\n",
    "        input_v_feature = input_v_feature.permute(0,3,1,2).contiguous()\n",
    "        _, v_decode = self.visual_encoder(input_v_feature)\n",
    "        v_decode = v_decode.squeeze(dim=-1).squeeze(dim=-1).contiguous()\n",
    "        v_decode = v_decode.reshape(batch_size, seq_len, -1)\n",
    "        v_decode = self.visual_reducer(v_decode)\n",
    "        \n",
    "        # decoding to ratings.\n",
    "        output, (_, _) = self.rating_decoder(l_decode)\n",
    "        output = self.rating_output(output)\n",
    "        output = output.squeeze(dim=-1)\n",
    "        output = output * input_mask\n",
    "        \n",
    "        a_decode = self.acoustic_encoder(input_a_feature)\n",
    "        output_a, (_, _) = self.rating_decoder_a(a_decode)\n",
    "        output_a = output_a.squeeze(dim=-1)\n",
    "        output_a = output_a * input_mask\n",
    "        \n",
    "        output_v, (_, _) = self.rating_decoder_v(v_decode)\n",
    "        output_v = output_v.squeeze(dim=-1)\n",
    "        output_v = output_v * input_mask\n",
    "        \n",
    "        output += output_a\n",
    "        output += output_v\n",
    "        \n",
    "        # get loss.\n",
    "        criterion = nn.MSELoss(reduction='sum')\n",
    "        loss = criterion(output, train_rating_labels)\n",
    "        \n",
    "        return loss, output\n",
    "    \n",
    "def evaluate(\n",
    "    test_dataloader, model, device, args,\n",
    "):\n",
    "    pbar = tqdm(test_dataloader, desc=\"Iteration\")\n",
    "    ccc = []\n",
    "    corr = []\n",
    "    outputs = []\n",
    "    total_loss = 0\n",
    "    data_num = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(pbar):\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            input_a_feature, input_l_feature, input_l_mask, input_l_segment_ids, \\\n",
    "                input_v_feature, rating_labels, seq_lens, input_mask = batch\n",
    "            input_a_feature = input_a_feature.to(device)\n",
    "            input_l_feature = input_l_feature.to(device)\n",
    "            input_l_mask = input_l_mask.to(device)\n",
    "            input_l_segment_ids = input_l_segment_ids.to(device)\n",
    "            input_v_feature = input_v_feature.to(device)\n",
    "            rating_labels = rating_labels.to(device)\n",
    "            seq_lens = seq_lens.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "\n",
    "            loss, output = \\\n",
    "                model(input_a_feature, input_l_feature, input_l_mask, input_l_segment_ids,\n",
    "                      input_v_feature, rating_labels, input_mask)\n",
    "            n_gpu = torch.cuda.device_count()\n",
    "            if n_gpu > 1:\n",
    "                loss = loss.mean() # mean() to average on multi-gpu.\n",
    "            total_loss += loss.data.cpu().detach().tolist()\n",
    "            data_num += torch.sum(seq_lens).tolist()\n",
    "            output_array = output.cpu().detach().numpy()\n",
    "            rating_labels_array = rating_labels.cpu().detach().numpy()\n",
    "            for i in range(0, input_a_feature.shape[0]):\n",
    "                ccc.append(eval_ccc(rating_labels_array[i][:int(seq_lens[i].tolist()[0])], output_array[i][:int(seq_lens[i].tolist()[0])]))\n",
    "                corr.append(pearsonr(output_array[i][:int(seq_lens[i].tolist()[0])], rating_labels_array[i][:int(seq_lens[i].tolist()[0])])[0])\n",
    "                outputs.append(output_array[i])\n",
    "        total_loss /= data_num\n",
    "    return total_loss, ccc, corr, outputs\n",
    "    \n",
    "def train(\n",
    "    train_dataloader, test_dataloader, model, optimizer, \n",
    "    device, args\n",
    "):\n",
    "    global_step = 0\n",
    "    best_ccc, best_corr = -1, -1\n",
    "    for _ in trange(int(args.num_train_epochs), desc=\"Epoch\"):\n",
    "        pbar = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "        for step, batch in enumerate(pbar):\n",
    "            model.train()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            input_a_feature, input_l_feature, input_l_mask, input_l_segment_ids, \\\n",
    "                input_v_feature, rating_labels, seq_lens, input_mask = batch\n",
    "            input_a_feature = input_a_feature.to(device)\n",
    "            input_l_feature = input_l_feature.to(device)\n",
    "            input_l_mask = input_l_mask.to(device)\n",
    "            input_l_segment_ids = input_l_segment_ids.to(device)\n",
    "            input_v_feature = input_v_feature.to(device)\n",
    "            rating_labels = rating_labels.to(device)\n",
    "            seq_lens = seq_lens.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            \n",
    "            loss, output = \\\n",
    "                model(input_a_feature, input_l_feature, input_l_mask, input_l_segment_ids,\n",
    "                      input_v_feature, rating_labels, input_mask)\n",
    "            n_gpu = torch.cuda.device_count()\n",
    "            if n_gpu > 1:\n",
    "                loss = loss.mean() # mean() to average on multi-gpu.\n",
    "            loss /= (torch.sum(seq_lens).tolist())\n",
    "            loss.backward() # uncomment this for actual run!\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            pbar.set_description(\"loss: %.4f\"%loss)\n",
    "            if args.is_tensorboard:\n",
    "                wandb.log({\"train_loss\": loss.cpu().detach().numpy()})\n",
    "            \n",
    "            if global_step%args.eval_interval == 0:\n",
    "                logger.info('Evaluating the model...')\n",
    "                # we need to evaluate!\n",
    "                loss, ccc, corr, outputs = evaluate(\n",
    "                    test_dataloader, model, device, args,\n",
    "                )\n",
    "                \n",
    "                if np.mean(ccc) > best_ccc:\n",
    "                    best_ccc = np.mean(ccc)\n",
    "                    # save best ccc models.\n",
    "                    if args.save_best_model:\n",
    "                        logger.info('Saving the new best model for ccc...')\n",
    "                        checkpoint = {'model': model.state_dict()}\n",
    "                        checkpoint_path = os.path.join(args.output_dir, \"best_ccc_pytorch_model.bin\")\n",
    "                        torch.save(checkpoint, checkpoint_path)\n",
    "                if np.mean(corr) > best_corr:\n",
    "                    best_corr = np.mean(corr)\n",
    "                    # save best corr models.\n",
    "                    if args.save_best_model:\n",
    "                        logger.info('Saving the new best model for corr...')\n",
    "                        checkpoint = {'model': model.state_dict()}\n",
    "                        checkpoint_path = os.path.join(args.output_dir, \"best_corr_pytorch_model.bin\")\n",
    "                        torch.save(checkpoint, checkpoint_path)\n",
    "                        \n",
    "                # Average statistics and print\n",
    "                stats = {'eval_loss': loss, 'corr': np.mean(corr), 'corr_std': np.std(corr),\n",
    "                         'ccc': np.mean(ccc), 'ccc_std': np.std(ccc), \n",
    "                         'best_ccc': best_ccc, 'best_corr': best_corr}\n",
    "                if args.is_tensorboard:\n",
    "                    wandb.log(stats)\n",
    "                logger.info(f'Evaluation results: {stats}')\n",
    "                \n",
    "            global_step +=  1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arg_parse():\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='multimodal emotion analysis argparse.')\n",
    "    # Experiment management:\n",
    "\n",
    "    parser.add_argument('--train_batch_size', type=int, default=6,\n",
    "                        help='Training batch size.')\n",
    "    parser.add_argument('--eval_batch_size', type=int, default=12,\n",
    "                        help='Evaluation batch size.')\n",
    "    parser.add_argument('--lr', type=float, default=1e-4,\n",
    "                        help='Learning rate.')\n",
    "    parser.add_argument('--warmup_proportion', type=float, default=0.1,\n",
    "                        help='Warmup period.')\n",
    "    parser.add_argument('--seed', type=int, default=42,\n",
    "                        help='Random seed.')\n",
    "    parser.add_argument('--num_train_epochs', type=float, default=3,\n",
    "                        help='Number of training epochs.')\n",
    "    parser.add_argument('--eval_interval', type=int, default=20,\n",
    "                        help='Evaluation interval in steps.')\n",
    "    parser.add_argument('--max_number_of_file', type=int, default=-1,\n",
    "                        help='Maybe we just want to test with a few number of files.')\n",
    "    \n",
    "    parser.add_argument('--resumed_from_file_path', type=str, default=\"\",\n",
    "                        help='Whether to resume for this file.')\n",
    "    parser.add_argument('--data_dir', type=str, default=\"../../SENDv1-data/\",\n",
    "                        help='Whether to resume for this file.')\n",
    "    parser.add_argument('--output_dir', type=str, default=\"../default_output_log/\",\n",
    "                        help='Whether to resume for this file.')\n",
    "    parser.add_argument(\"--is_tensorboard\",\n",
    "                        default=False,\n",
    "                        action='store_true',\n",
    "                        help=\"Whether to use tensorboard.\")\n",
    "    parser.add_argument(\"--save_best_model\",\n",
    "                        default=False,\n",
    "                        action='store_true',\n",
    "                        help=\"Whether to save the best model during eval.\")\n",
    "    parser.add_argument(\"--eval_only\",\n",
    "                        default=False,\n",
    "                        action='store_true',\n",
    "                        help=\"Whether we are evaluating the model only.\")\n",
    "    parser.add_argument(\"--debug_only\",\n",
    "                        default=False,\n",
    "                        action='store_true',\n",
    "                        help=\"Whether we are debugging the code only.\")\n",
    "    parser.add_argument(\"--use_target_ratings\",\n",
    "                        default=False,\n",
    "                        action='store_true',\n",
    "                        help=\"Whether to use target ratings from the dataset.\")\n",
    "\n",
    "    parser.set_defaults(\n",
    "        # Exp management:\n",
    "        seed=42,\n",
    "    )\n",
    "    try:\n",
    "        get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "        args = parser.parse_args([])\n",
    "    except:\n",
    "        args = parser.parse_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Loading arguments\n",
    "    args = arg_parse()\n",
    "    try:        \n",
    "        get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "        # Experiment management:\n",
    "        args.train_batch_size=1\n",
    "        args.eval_batch_size=1\n",
    "        args.lr=8e-5\n",
    "        args.seed=42\n",
    "        args.is_tensorboard=True # Let us try this!\n",
    "        args.output_dir=\"../default_output_log/\"\n",
    "        is_jupyter = True\n",
    "    except:\n",
    "        is_jupyter = False\n",
    "        \n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    if n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "    # Create output directory if not exists.\n",
    "    pathlib.Path(args.output_dir).mkdir(parents=True, exist_ok=True) \n",
    "    \n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO, \n",
    "        format='%(asctime)s %(levelname)-8s %(message)s', \n",
    "        datefmt='%a, %d %b %Y %H:%M:%S', \n",
    "        filename=os.path.join(args.output_dir, \"training.log\"),\n",
    "    )\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.getLogger().addHandler(logging.StreamHandler(os.sys.stdout))\n",
    "    \n",
    "    logger.info(\"Training the model with the following parameters: \")\n",
    "    logger.info(args)\n",
    "    \n",
    "    if args.is_tensorboard and not is_jupyter:\n",
    "        logger.warning(\"Enabling wandb for tensorboard logging...\")\n",
    "        run = wandb.init(project=\"SEND-Multimodal\", entity=\"wuzhengx\")\n",
    "        run_name = wandb.run.name\n",
    "        wandb.config.update(args)\n",
    "    else:\n",
    "        wandb = None\n",
    "    \n",
    "    # We don't allow flexibility here..\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(\n",
    "#         \"bert-base-uncased\",\n",
    "#         use_fast=False,\n",
    "#         cache_dir=\"../.huggingface_cache/\"\n",
    "#     )\n",
    "    \n",
    "#     train_SEND_features = None\n",
    "#     test_SEND_features = None\n",
    "    \n",
    "#     if args.use_target_ratings:\n",
    "#         logger.info(\"WARNING: use_target_ratings is setting to TRUE.\")\n",
    "#         modality_dir_map = {\"acoustic\": \"acoustic-egemaps\",  \n",
    "#                             \"linguistic\": \"linguistic-word-level\", # we don't load features\n",
    "#                             \"visual\": \"image-raw\", # image is nested,\n",
    "#                             \"target\": \"target\"}\n",
    "#         preprocess = {\n",
    "#             'acoustic': lambda df : df.loc[:,' F0semitoneFrom27.5Hz_sma3nz_amean':' equivalentSoundLevel_dBp'],\n",
    "#             'acoustic_timer': lambda df : df.loc[:,' frameTime'],\n",
    "#             'linguistic': lambda df : df.loc[:,'word'],\n",
    "#             'linguistic_timer': lambda df : df.loc[:,'time-offset'],\n",
    "#             'target': lambda df : ((df.loc[:,' rating'] / 0.5) - 1.0),\n",
    "#             'target_timer': lambda df : df.loc[:,'time'],\n",
    "#         }\n",
    "#     else:\n",
    "#         logger.info(\"WARNING: use_target_ratings is setting to FALSE.\")\n",
    "#         modality_dir_map = {\"acoustic\": \"acoustic-egemaps\",  \n",
    "#                             \"linguistic\": \"linguistic-word-level\", # we don't load features\n",
    "#                             \"visual\": \"image-raw\", # image is nested,\n",
    "#                             \"target\": \"observer_EWE\"}\n",
    "#         preprocess = {\n",
    "#             'acoustic': lambda df : df.loc[:,' F0semitoneFrom27.5Hz_sma3nz_amean':' equivalentSoundLevel_dBp'],\n",
    "#             'acoustic_timer': lambda df : df.loc[:,' frameTime'],\n",
    "#             'linguistic': lambda df : df.loc[:,'word'],\n",
    "#             'linguistic_timer': lambda df : df.loc[:,'time-offset'],\n",
    "#             'target': lambda df : ((df.loc[:,'evaluatorWeightedEstimate'] / 50.0) - 1.0),\n",
    "#             'target_timer': lambda df : df.loc[:,'time'],\n",
    "#         }\n",
    "#     if not args.eval_only:\n",
    "#         # Training data loading \n",
    "#         train_modalities_data_dir = os.path.join(args.data_dir, \"features/Train/\")\n",
    "#         train_target_data_dir = os.path.join(args.data_dir, \"ratings/Train\")\n",
    "\n",
    "#         test_modalities_data_dir = os.path.join(args.data_dir, \"features/Valid/\")\n",
    "#         test_target_data_dir = os.path.join(args.data_dir, \"ratings/Valid\")\n",
    "        \n",
    "#         train_SEND_features = preprocess_SEND_files(\n",
    "#             train_modalities_data_dir,\n",
    "#             train_target_data_dir,\n",
    "#             args.use_target_ratings,\n",
    "#             modality_dir_map=modality_dir_map,\n",
    "#             preprocess=preprocess,\n",
    "#             linguistic_tokenizer=tokenizer,\n",
    "#             max_number_of_file=args.max_number_of_file\n",
    "#         )\n",
    "#         if args.debug_only:\n",
    "#             logger.info(\"WARNING: Debugging only. Evaluate and Train datasets are the same.\")\n",
    "#             test_SEND_features = copy.deepcopy(train_SEND_features)\n",
    "#         else:\n",
    "#             test_SEND_features = preprocess_SEND_files(\n",
    "#                 test_modalities_data_dir,\n",
    "#                 test_target_data_dir,\n",
    "#                 args.use_target_ratings,\n",
    "#                 modality_dir_map=modality_dir_map,\n",
    "#                 preprocess=preprocess,\n",
    "#                 linguistic_tokenizer=tokenizer,\n",
    "#             )\n",
    "        \n",
    "#     else:\n",
    "#         test_modalities_data_dir = os.path.join(args.data_dir, \"features/Test/\")\n",
    "#         test_target_data_dir = os.path.join(args.data_dir, \"ratings/Test\")\n",
    "    \n",
    "#         test_SEND_features = preprocess_SEND_files(\n",
    "#             test_modalities_data_dir,\n",
    "#             test_target_data_dir,\n",
    "#             args,\n",
    "#             modality_dir_map=modality_dir_map,\n",
    "#             preprocess=preprocess,\n",
    "#             linguistic_tokenizer=tokenizer,\n",
    "#             max_number_of_file=args.max_number_of_file\n",
    "#         )\n",
    "    train_data = torch.load('./train_data.pt')\n",
    "    test_data = torch.load('./test_data.pt')\n",
    "    logger.info(\"Finish Loading Datasets...\")\n",
    "    \n",
    "    if not args.eval_only:\n",
    "        # Initialize all the datasets\n",
    "#         train_input_a_feature = torch.stack([video_struct[\"a_feature\"] for video_struct in train_SEND_features]).float()\n",
    "#         train_input_l_feature = torch.stack([video_struct[\"l_feature\"] for video_struct in train_SEND_features])\n",
    "#         train_input_l_mask = torch.stack([video_struct[\"l_mask\"] for video_struct in train_SEND_features])\n",
    "#         train_input_l_segment_ids = torch.stack([video_struct[\"l_segment_ids\"] for video_struct in train_SEND_features])\n",
    "#         train_input_v_feature = torch.stack([video_struct[\"v_feature\"] for video_struct in train_SEND_features]).float()\n",
    "#         train_rating_labels = torch.stack([video_struct[\"rating\"] for video_struct in train_SEND_features]).float()\n",
    "#         train_seq_lens = torch.tensor([[video_struct[\"seq_len\"]] for video_struct in train_SEND_features]).float()\n",
    "#         train_input_mask = torch.stack([video_struct[\"input_mask\"] for video_struct in train_SEND_features])\n",
    "\n",
    "#         test_input_a_feature = torch.stack([video_struct[\"a_feature\"] for video_struct in test_SEND_features]).float()\n",
    "#         test_input_l_feature = torch.stack([video_struct[\"l_feature\"] for video_struct in test_SEND_features])\n",
    "#         test_input_l_mask = torch.stack([video_struct[\"l_mask\"] for video_struct in test_SEND_features])\n",
    "#         test_input_l_segment_ids = torch.stack([video_struct[\"l_segment_ids\"] for video_struct in test_SEND_features])\n",
    "#         test_input_v_feature = torch.stack([video_struct[\"v_feature\"] for video_struct in test_SEND_features]).float()\n",
    "#         test_rating_labels = torch.stack([video_struct[\"rating\"] for video_struct in test_SEND_features]).float()\n",
    "#         test_seq_lens = torch.tensor([[video_struct[\"seq_len\"]] for video_struct in test_SEND_features]).float()\n",
    "#         test_input_mask = torch.stack([video_struct[\"input_mask\"] for video_struct in test_SEND_features])\n",
    "\n",
    "#         train_data = TensorDataset(\n",
    "#             train_input_a_feature, \n",
    "#             train_input_l_feature, train_input_l_mask, train_input_l_segment_ids,\n",
    "#             train_input_v_feature, train_rating_labels, train_seq_lens, train_input_mask\n",
    "#         )\n",
    "        train_sampler = RandomSampler(train_data)\n",
    "        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size)\n",
    "\n",
    "#         test_data = TensorDataset(\n",
    "#             test_input_a_feature, \n",
    "#             test_input_l_feature, test_input_l_mask, test_input_l_segment_ids,\n",
    "#             test_input_v_feature, test_rating_labels, test_seq_lens, test_input_mask\n",
    "#         )\n",
    "        test_dataloader = DataLoader(test_data, batch_size=args.eval_batch_size, shuffle=False)\n",
    "    else:\n",
    "        logger.info(\"Not implemented...\")\n",
    "        \n",
    "    if not args.eval_only:\n",
    "        # Init model with optimizer.\n",
    "        model = MultimodalEmotionPrediction()\n",
    "        no_decay = ['bias', 'gamma', 'beta']\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in model.named_parameters() \n",
    "                if not any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.01},\n",
    "            {'params': [p for n, p in model.named_parameters() \n",
    "                if any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.0}\n",
    "            ]\n",
    "        num_train_steps = int(\n",
    "            len(train_data) / args.train_batch_size * args.num_train_epochs)\n",
    "        # We use the default BERT optimz to do gradient descent.\n",
    "        # optimizer = BERTAdam(optimizer_parameters,\n",
    "        #                     lr=args.lr,\n",
    "        #                     warmup=args.warmup_proportion,\n",
    "        #                     t_total=num_train_steps)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-4)\n",
    "        # Determine the device.\n",
    "        if not torch.cuda.is_available() or is_jupyter:\n",
    "            device = torch.device(\"cpu\")\n",
    "            n_gpu = -1\n",
    "        else:\n",
    "            device = torch.device(\"cuda\")\n",
    "            n_gpu = torch.cuda.device_count()\n",
    "\n",
    "        if n_gpu > 1:\n",
    "            model = torch.nn.DataParallel(model)\n",
    "        model = model.to(device)\n",
    "\n",
    "        train(\n",
    "            train_dataloader, test_dataloader, model, optimizer,\n",
    "            device, args\n",
    "        )\n",
    "    else:\n",
    "        logger.info(\"Not implemented...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
