{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Script of Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_multimodal_time_series import *\n",
    "from collections import OrderedDict\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_target_ratings=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing LinguisticEncoderBERT: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing LinguisticEncoderBERT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LinguisticEncoderBERT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the model from:  ../target/best_ccc_pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# loading model from saved model.\n",
    "model = MultimodalEmotionPrediction()\n",
    "new_state_dict = OrderedDict()\n",
    "DEVICE = torch.device('cpu')   # 'cpu' in this case\n",
    "if use_target_ratings:\n",
    "    model_path = \"../target/best_ccc_pytorch_model.bin\"\n",
    "else:\n",
    "    model_path = \"../observer/best_ccc_pytorch_model.bin\"\n",
    "print(\"loading the model from: \", model_path)\n",
    "state_dict = torch.load(model_path, map_location=DEVICE)[\"model\"]\n",
    "for k, v in state_dict.items():\n",
    "    name = k[7:] # remove `module.`\n",
    "    new_state_dict[name] = v\n",
    "model.load_state_dict(new_state_dict)\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: use_target_ratings is setting to TRUE.\n"
     ]
    }
   ],
   "source": [
    "if use_target_ratings:\n",
    "    print(\"WARNING: use_target_ratings is setting to TRUE.\")\n",
    "    modality_dir_map = {\"acoustic\": \"acoustic-egemaps\",  \n",
    "                        \"linguistic\": \"linguistic-word-level\", # we don't load features\n",
    "                        \"visual\": \"image-raw\", # image is nested,\n",
    "                        \"target\": \"target\"}\n",
    "    preprocess = {\n",
    "        'acoustic': lambda df : df.loc[:,' F0semitoneFrom27.5Hz_sma3nz_amean':' equivalentSoundLevel_dBp'],\n",
    "        'acoustic_timer': lambda df : df.loc[:,' frameTime'],\n",
    "        'linguistic': lambda df : df.loc[:,'word'],\n",
    "        'linguistic_timer': lambda df : df.loc[:,'time-offset'],\n",
    "        'target': lambda df : ((df.loc[:,' rating'] / 0.5) - 1.0),\n",
    "        'target_timer': lambda df : df.loc[:,'time'],\n",
    "    }\n",
    "else:\n",
    "    modality_dir_map = {\"acoustic\": \"acoustic-egemaps\",  \n",
    "                        \"linguistic\": \"linguistic-word-level\", # we don't load features\n",
    "                        \"visual\": \"image-raw\", # image is nested,\n",
    "                        \"target\": \"observer_EWE\"}\n",
    "    preprocess = {\n",
    "        'acoustic': lambda df : df.loc[:,' F0semitoneFrom27.5Hz_sma3nz_amean':' equivalentSoundLevel_dBp'],\n",
    "        'acoustic_timer': lambda df : df.loc[:,' frameTime'],\n",
    "        'linguistic': lambda df : df.loc[:,'word'],\n",
    "        'linguistic_timer': lambda df : df.loc[:,'time-offset'],\n",
    "        'target': lambda df : ((df.loc[:,'evaluatorWeightedEstimate'] / 50.0) - 1.0),\n",
    "        'target_timer': lambda df : df.loc[:,'time'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_target_ratings:\n",
    "    output_dir = \"../data-files/target/\"\n",
    "else:\n",
    "    output_dir = \"../data-files/observer/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    use_fast=False,\n",
    "    cache_dir=\"../.huggingface_cache/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading all the data partitions.\n",
    "data_dir = \"../../SENDv1-data/\"\n",
    "train_modalities_data_dir = os.path.join(data_dir, \"features/Train/\")\n",
    "train_target_data_dir = os.path.join(data_dir, \"ratings/Train\")\n",
    "train_SEND_features = preprocess_SEND_files(\n",
    "    train_modalities_data_dir,\n",
    "    train_target_data_dir,\n",
    "    use_target_ratings,\n",
    "    modality_dir_map=modality_dir_map,\n",
    "    preprocess=preprocess,\n",
    "    linguistic_tokenizer=tokenizer,\n",
    "    max_number_of_file=-1\n",
    ")\n",
    "\n",
    "dev_modalities_data_dir = os.path.join(data_dir, \"features/Valid/\")\n",
    "dev_target_data_dir = os.path.join(data_dir, \"ratings/Valid\")\n",
    "dev_SEND_features = preprocess_SEND_files(\n",
    "    dev_modalities_data_dir,\n",
    "    dev_target_data_dir,\n",
    "    use_target_ratings,\n",
    "    modality_dir_map=modality_dir_map,\n",
    "    preprocess=preprocess,\n",
    "    linguistic_tokenizer=tokenizer,\n",
    "    max_number_of_file=-1\n",
    ")\n",
    "\n",
    "test_modalities_data_dir = os.path.join(data_dir, \"features/Test/\")\n",
    "test_target_data_dir = os.path.join(data_dir, \"ratings/Test\")\n",
    "test_SEND_features = preprocess_SEND_files(\n",
    "    test_modalities_data_dir,\n",
    "    test_target_data_dir,\n",
    "    use_target_ratings,\n",
    "    modality_dir_map=modality_dir_map,\n",
    "    preprocess=preprocess,\n",
    "    linguistic_tokenizer=tokenizer,\n",
    "    max_number_of_file=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put dataset into correct format.\n",
    "train_video_id = [video_struct[\"video_id\"] for video_struct in train_SEND_features]\n",
    "train_input_a_feature = torch.stack([video_struct[\"a_feature\"] for video_struct in train_SEND_features]).float()\n",
    "train_input_l_feature = torch.stack([video_struct[\"l_feature\"] for video_struct in train_SEND_features])\n",
    "train_input_l_mask = torch.stack([video_struct[\"l_mask\"] for video_struct in train_SEND_features])\n",
    "train_input_l_segment_ids = torch.stack([video_struct[\"l_segment_ids\"] for video_struct in train_SEND_features])\n",
    "train_input_v_feature = torch.stack([video_struct[\"v_feature\"] for video_struct in train_SEND_features]).float()\n",
    "train_rating_labels = torch.stack([video_struct[\"rating\"] for video_struct in train_SEND_features]).float()\n",
    "train_seq_lens = torch.tensor([[video_struct[\"seq_len\"]] for video_struct in train_SEND_features]).float()\n",
    "train_input_mask = torch.stack([video_struct[\"input_mask\"] for video_struct in train_SEND_features])\n",
    "train_data = TensorDataset(\n",
    "    train_input_a_feature, \n",
    "    train_input_l_feature, train_input_l_mask, train_input_l_segment_ids,\n",
    "    train_input_v_feature, train_rating_labels, train_seq_lens, train_input_mask\n",
    ")\n",
    "train_dataloader = DataLoader(train_data, batch_size=1, shuffle=False)\n",
    "\n",
    "dev_video_id = [video_struct[\"video_id\"] for video_struct in dev_SEND_features]\n",
    "dev_input_a_feature = torch.stack([video_struct[\"a_feature\"] for video_struct in dev_SEND_features]).float()\n",
    "dev_input_l_feature = torch.stack([video_struct[\"l_feature\"] for video_struct in dev_SEND_features])\n",
    "dev_input_l_mask = torch.stack([video_struct[\"l_mask\"] for video_struct in dev_SEND_features])\n",
    "dev_input_l_segment_ids = torch.stack([video_struct[\"l_segment_ids\"] for video_struct in dev_SEND_features])\n",
    "dev_input_v_feature = torch.stack([video_struct[\"v_feature\"] for video_struct in dev_SEND_features]).float()\n",
    "dev_rating_labels = torch.stack([video_struct[\"rating\"] for video_struct in dev_SEND_features]).float()\n",
    "dev_seq_lens = torch.tensor([[video_struct[\"seq_len\"]] for video_struct in dev_SEND_features]).float()\n",
    "dev_input_mask = torch.stack([video_struct[\"input_mask\"] for video_struct in dev_SEND_features])\n",
    "dev_data = TensorDataset(\n",
    "    dev_input_a_feature, \n",
    "    dev_input_l_feature, dev_input_l_mask, dev_input_l_segment_ids,\n",
    "    dev_input_v_feature, dev_rating_labels, dev_seq_lens, dev_input_mask\n",
    ")\n",
    "dev_dataloader = DataLoader(dev_data, batch_size=1, shuffle=False)\n",
    "\n",
    "test_video_id = [video_struct[\"video_id\"] for video_struct in test_SEND_features]\n",
    "test_input_a_feature = torch.stack([video_struct[\"a_feature\"] for video_struct in test_SEND_features]).float()\n",
    "test_input_l_feature = torch.stack([video_struct[\"l_feature\"] for video_struct in test_SEND_features])\n",
    "test_input_l_mask = torch.stack([video_struct[\"l_mask\"] for video_struct in test_SEND_features])\n",
    "test_input_l_segment_ids = torch.stack([video_struct[\"l_segment_ids\"] for video_struct in test_SEND_features])\n",
    "test_input_v_feature = torch.stack([video_struct[\"v_feature\"] for video_struct in test_SEND_features]).float()\n",
    "test_rating_labels = torch.stack([video_struct[\"rating\"] for video_struct in test_SEND_features]).float()\n",
    "test_seq_lens = torch.tensor([[video_struct[\"seq_len\"]] for video_struct in test_SEND_features]).float()\n",
    "test_input_mask = torch.stack([video_struct[\"input_mask\"] for video_struct in test_SEND_features])\n",
    "test_data = TensorDataset(\n",
    "    test_input_a_feature, \n",
    "    test_input_l_feature, test_input_l_mask, test_input_l_segment_ids,\n",
    "    test_input_v_feature, test_rating_labels, test_seq_lens, test_input_mask\n",
    ")\n",
    "test_dataloader = DataLoader(test_data, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ablation(\n",
    "    video_id, dataloader, model, condition=\"A,V,L\"\n",
    "):\n",
    "    ret = {}\n",
    "    video_index = 0\n",
    "    pbar = tqdm(dataloader, desc=\"videos\")\n",
    "    for step, batch in enumerate(pbar):\n",
    "        vid_id = video_id[video_index]\n",
    "        ret[vid_id] = {}\n",
    "        # print(f\"analyzing ablation studies on video_id={vid_id}\")\n",
    "        input_a_feature, input_l_feature, input_l_mask, input_l_segment_ids, \\\n",
    "            input_v_feature, rating_labels, seq_lens, input_mask = batch\n",
    "        # based one condition, we need to mask out some channels!\n",
    "        if \"A\" not in condition:\n",
    "            input_a_feature = torch.zeros_like(input_a_feature)\n",
    "        if \"V\" not in condition:\n",
    "            input_v_feature = torch.zeros_like(input_v_feature)\n",
    "        if \"L\" not in condition:\n",
    "            input_l_feature = torch.zeros_like(input_l_feature)\n",
    "        _, output = \\\n",
    "            model(input_a_feature, input_l_feature, input_l_mask, input_l_segment_ids,\n",
    "                  input_v_feature, rating_labels, input_mask)\n",
    "        seq_l = int(seq_lens[0].tolist()[0])\n",
    "        pred = output[0][:seq_l].cpu().detach().numpy()\n",
    "        true = rating_labels[0][:seq_l].cpu().detach().numpy()\n",
    "        ccc = eval_ccc(pred, true)\n",
    "        ret[vid_id][\"pred\"] = pred\n",
    "        ret[vid_id][\"true\"] = true\n",
    "        video_index += 1\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "videos:   0%|          | 0/114 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyzing results for condition:  A,V,L\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "videos: 100%|██████████| 114/114 [04:25<00:00,  2.33s/it]\n",
      "videos: 100%|██████████| 40/40 [01:07<00:00,  1.70s/it]\n",
      "videos: 100%|██████████| 39/39 [01:15<00:00,  1.93s/it]\n",
      "videos:   0%|          | 0/114 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyzing results for condition:  A,V\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "videos: 100%|██████████| 114/114 [04:17<00:00,  2.26s/it]\n",
      "videos: 100%|██████████| 40/40 [00:59<00:00,  1.49s/it]\n",
      "videos: 100%|██████████| 39/39 [00:58<00:00,  1.51s/it]\n",
      "videos:   0%|          | 0/114 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyzing results for condition:  A,L\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "videos: 100%|██████████| 114/114 [04:06<00:00,  2.16s/it]\n",
      "videos: 100%|██████████| 40/40 [01:11<00:00,  1.78s/it]\n",
      "videos: 100%|██████████| 39/39 [01:04<00:00,  1.65s/it]\n",
      "videos:   0%|          | 0/114 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyzing results for condition:  V,L\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "videos: 100%|██████████| 114/114 [04:12<00:00,  2.22s/it]\n",
      "videos: 100%|██████████| 40/40 [01:08<00:00,  1.70s/it]\n",
      "videos: 100%|██████████| 39/39 [01:15<00:00,  1.93s/it]\n",
      "videos:   0%|          | 0/114 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyzing results for condition:  A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "videos: 100%|██████████| 114/114 [05:44<00:00,  3.02s/it]\n",
      "videos: 100%|██████████| 40/40 [01:33<00:00,  2.35s/it]\n",
      "videos: 100%|██████████| 39/39 [01:43<00:00,  2.65s/it]\n",
      "videos:   0%|          | 0/114 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyzing results for condition:  V\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "videos: 100%|██████████| 114/114 [07:04<00:00,  3.72s/it]\n",
      "videos: 100%|██████████| 40/40 [01:46<00:00,  2.66s/it]\n",
      "videos: 100%|██████████| 39/39 [01:35<00:00,  2.44s/it]\n",
      "videos:   0%|          | 0/114 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyzing results for condition:  L\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "videos: 100%|██████████| 114/114 [05:38<00:00,  2.97s/it]\n",
      "videos: 100%|██████████| 40/40 [01:31<00:00,  2.28s/it]\n",
      "videos: 100%|██████████| 39/39 [01:40<00:00,  2.58s/it]\n"
     ]
    }
   ],
   "source": [
    "conditions = [\"A,V,L\", \"A,V\", \"A,L\", \"V,L\", \"A\", \"V\", \"L\"]\n",
    "mega_results = {}\n",
    "for condition in conditions:\n",
    "    print(\"analyzing results for condition: \", condition)\n",
    "    train_results = evaluate_ablation(\n",
    "        train_video_id, train_dataloader, model,\n",
    "        condition=condition\n",
    "    )\n",
    "    \n",
    "    dev_results = evaluate_ablation(\n",
    "        dev_video_id, dev_dataloader, model,\n",
    "        condition=condition\n",
    "    )\n",
    "\n",
    "    test_results = evaluate_ablation(\n",
    "        test_video_id, test_dataloader, model,\n",
    "        condition=condition\n",
    "    )\n",
    "    mega_results[condition] = {}\n",
    "    for k,v in train_results.items():\n",
    "        mega_results[condition][k] = v\n",
    "    for k,v in dev_results.items():\n",
    "        mega_results[condition][k] = v\n",
    "    for k,v in test_results.items():\n",
    "        mega_results[condition][k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output dir:  ../data-files/target/\n"
     ]
    }
   ],
   "source": [
    "print(\"output dir: \", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each video, we are creating a file to save ratings for all conditions.\n",
    "for video in mega_results[\"A,V,L\"].keys():\n",
    "    with open(os.path.join(output_dir, f\"{video}.csv\"), \"w\") as csv_file:\n",
    "        writer = csv.writer(csv_file, delimiter=',')\n",
    "        headers = [c for c in conditions]\n",
    "        headers += [\"actual\"]\n",
    "        writer.writerow(headers)\n",
    "        s_len = len(mega_results[\"A,V,L\"][video][\"pred\"])\n",
    "        for i in range(s_len): # write line by line.\n",
    "            row = []\n",
    "            for condition in conditions:\n",
    "                norm_r = (mega_results[condition][video][\"pred\"][i]+1.0)/2.0\n",
    "                row.append(norm_r)\n",
    "            norm_r = (mega_results[condition][video][\"true\"][i]+1.0)/2.0\n",
    "            row.append(norm_r)\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data-files/train_ids.csv\", \"w\") as csv_file:\n",
    "    writer = csv.writer(csv_file, delimiter=',')\n",
    "    headers = [\"vid_id\"]\n",
    "    writer.writerow(headers)\n",
    "    for vid_id in train_video_id:\n",
    "        writer.writerow([vid_id])\n",
    "with open(\"../data-files/dev_ids.csv\", \"w\") as csv_file:\n",
    "    writer = csv.writer(csv_file, delimiter=',')\n",
    "    headers = [\"vid_id\"]\n",
    "    writer.writerow(headers)\n",
    "    for vid_id in dev_video_id:\n",
    "        writer.writerow([vid_id])\n",
    "with open(\"../data-files/test_ids.csv\", \"w\") as csv_file:\n",
    "    writer = csv.writer(csv_file, delimiter=',')\n",
    "    headers = [\"vid_id\"]\n",
    "    writer.writerow(headers)\n",
    "    for vid_id in test_video_id:\n",
    "        writer.writerow([vid_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate with Hebrew Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_target_ratings = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing LinguisticEncoderBERT: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing LinguisticEncoderBERT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LinguisticEncoderBERT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the model from:  ../target/best_ccc_pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# loading model from saved model.\n",
    "model = MultimodalEmotionPrediction()\n",
    "new_state_dict = OrderedDict()\n",
    "DEVICE = torch.device('cpu')   # 'cpu' in this case\n",
    "if use_target_ratings:\n",
    "    model_path = \"../target/best_ccc_pytorch_model.bin\"\n",
    "else:\n",
    "    model_path = \"../observer/best_ccc_pytorch_model.bin\"\n",
    "print(\"loading the model from: \", model_path)\n",
    "state_dict = torch.load(model_path, map_location=DEVICE)[\"model\"]\n",
    "for k, v in state_dict.items():\n",
    "    name = k[7:] # remove `module.`\n",
    "    new_state_dict[name] = v\n",
    "model.load_state_dict(new_state_dict)\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_target_ratings:\n",
    "    output_dir = \"../data-files/target_hebrew/\"\n",
    "else:\n",
    "    output_dir = \"../data-files/observer_hebrew/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_HEBREW_files(\n",
    "    data_dir, # Multitmodal X\n",
    "    time_window_in_sec=4.0,\n",
    "    modality_dir_map = {\"acoustic\": \"acoustic-egemaps\",  \n",
    "                        \"linguistic\": \"linguistic-word-level\", # we don't load features\n",
    "                        \"visual\": \"image-raw\", # image is nested,\n",
    "                        \"target\": \"observer_EWE\",\n",
    "                       },\n",
    "    preprocess= {'acoustic': lambda df : df.loc[:,' F0semitoneFrom27.5Hz_sma3nz_amean':' equivalentSoundLevel_dBp'],\n",
    "                 'acoustic_timer': lambda df : df.loc[:,' frameTime'],\n",
    "                 'linguistic': lambda df : df.loc[:,'word'],\n",
    "                 'linguistic_timer': lambda df : df.loc[:,'time-offset'],\n",
    "                 'target': lambda df : ((df.loc[:,'evaluatorWeightedEstimate'] / 50.0) - 1.0),\n",
    "                 'target_timer': lambda df : df.loc[:,'time'],\n",
    "                },\n",
    "    pad_symbol=0,\n",
    "    max_number_of_file=-1\n",
    "):\n",
    "    SEND_videos = []\n",
    "    \n",
    "    # basically, let us gett all the video ids?\n",
    "    a_ids = [f.split(\"_\")[0]+\"_\"+f.split(\"_\")[1] \n",
    "             for f in listdir(os.path.join(data_dir, modality_dir_map[\"acoustic\"])) \n",
    "             if isfile(os.path.join(data_dir, modality_dir_map[\"acoustic\"], f))]\n",
    "    v_ids = [f.split(\"_\")[0]+\"_\"+f.split(\"_\")[1] \n",
    "             for f in listdir(os.path.join(data_dir, modality_dir_map[\"visual\"])) \n",
    "             if f != \".DS_Store\"]\n",
    "\n",
    "    if max_number_of_file != -1:\n",
    "        logger.info(f\"WARNING: Only loading #{max_number_of_file} videos.\")\n",
    "    max_seq_len = -1\n",
    "    video_count = 0\n",
    "    for video_id in a_ids: # pick any one!\n",
    "        if max_number_of_file != -1 and video_count >= max_number_of_file:\n",
    "            break # we enforce!\n",
    "        if video_count > 1 and video_count%100 == 0:\n",
    "            logger.info(f\"Processed #{len(SEND_videos)} videos.\")\n",
    "            # logger.info(SEND_videos[-1])\n",
    "        \n",
    "        # we need to fix this to get features aligned.\n",
    "        \n",
    "        # Step 1: Load rating data, and we can get window partitioned according to our interval.\n",
    "        a_file = os.path.join(data_dir, modality_dir_map[\"acoustic\"], f\"{video_id}_acousticFeatures.csv\")\n",
    "        a_df = pd.read_csv(a_file)\n",
    "        a_features = np.array(preprocess[\"acoustic\"](a_df))\n",
    "        a_timestamps = np.array(preprocess[\"acoustic_timer\"](a_df))\n",
    "        windows = []\n",
    "        number_of_window = int(max(a_timestamps)//time_window_in_sec)\n",
    "        for i in range(0, number_of_window):\n",
    "            windows += [(i*time_window_in_sec, (i+1)*time_window_in_sec)]\n",
    "        if max(a_timestamps) > (i+1)*time_window_in_sec:\n",
    "            windows += [((i+1)*time_window_in_sec, max(a_timestamps))]\n",
    "        # [(0, 5], (5, 10], ...]\n",
    "\n",
    "        # acoustic features process\n",
    "        a_file = os.path.join(data_dir, modality_dir_map[\"acoustic\"], f\"{video_id}_acousticFeatures.csv\")\n",
    "        a_df = pd.read_csv(a_file)\n",
    "        a_features = np.array(preprocess[\"acoustic\"](a_df))\n",
    "        a_timestamps = np.array(preprocess[\"acoustic_timer\"](a_df))\n",
    "        a_feature_dim = a_features.shape[1]\n",
    "        assert a_features.shape[0] == a_timestamps.shape[0]\n",
    "        sampled_a_features_raw = [[] for i in range(len(windows))]\n",
    "        for i in range(0, a_timestamps.shape[0]):\n",
    "            # using mod to hash to the correct bucket.\n",
    "            hash_in_window = int(a_timestamps[i]//time_window_in_sec)\n",
    "            if hash_in_window >= len(windows):\n",
    "                continue # we cannot predict after ratings max.\n",
    "            sampled_a_features_raw[hash_in_window].append(a_features[i])\n",
    "        sampled_a_features = []\n",
    "        for window in sampled_a_features_raw:\n",
    "            # only acoustic need to consider this I think.\n",
    "            if len(window) == 0:\n",
    "                collate_window = np.zeros(a_feature_dim)\n",
    "            else:\n",
    "                collate_window = np.mean(np.array(window), axis=0)\n",
    "            sampled_a_features.append(collate_window)\n",
    "\n",
    "        # visual features process\n",
    "        # for visual, we actually need to active control what image we load, we\n",
    "        # cannot just load all images, it will below memory.\n",
    "        fps=30 # We may need to dynamically figure out this number?\n",
    "        frame_names = []\n",
    "        for f in listdir(os.path.join(data_dir, modality_dir_map[\"visual\"], video_id)):\n",
    "            if \".jpg\" in f:\n",
    "                frame_names += [(int(f.split(\"_\")[0][5:])*(1.0/fps), f)]\n",
    "        frame_names.sort(key=lambda x:x[0])\n",
    "        sampled_v_features_raw = [[] for i in range(len(windows))]\n",
    "        for f in frame_names:\n",
    "            # using mod to hash to the correct bucket.\n",
    "            hash_in_window = int(f[0]//time_window_in_sec)\n",
    "            if hash_in_window >= len(windows):\n",
    "                continue # we cannot predict after ratings max.\n",
    "            sampled_v_features_raw[hash_in_window].append(f)\n",
    "\n",
    "        sampled_v_features = []\n",
    "        for window in sampled_v_features_raw:\n",
    "            if len(window) == 0:\n",
    "                f_data = np.zeros((224,224,3))\n",
    "            else:\n",
    "                # we collate by using the last frame in the time window.\n",
    "                f = window[-1]\n",
    "                f_path = os.path.join(data_dir, modality_dir_map[\"visual\"], video_id, f[1])\n",
    "                f_image = Image.open(f_path)\n",
    "                f_data = asarray(f_image)\n",
    "            sampled_v_features.append(f_data)\n",
    "        \n",
    "        max_window_cutoff_a = int(max(a_timestamps)//time_window_in_sec)\n",
    "        max_window_cutoff_v = int(frame_names[-1][0]//time_window_in_sec)\n",
    "        max_window_cutoff = min([max_window_cutoff_a, max_window_cutoff_v])\n",
    "        sampled_a_features = sampled_a_features[:max_window_cutoff]\n",
    "        sampled_v_features = sampled_v_features[:max_window_cutoff]\n",
    "        \n",
    "        video_struct = {\n",
    "            \"video_id\": video_id,\n",
    "            \"a_feature\": sampled_a_features,\n",
    "            \"v_feature\": sampled_v_features,\n",
    "            \"seq_len\": len(sampled_a_features),\n",
    "            \"input_mask\": np.ones(len(sampled_a_features)).tolist()\n",
    "        }\n",
    "        video_count += 1\n",
    "        SEND_videos += [video_struct]\n",
    "        if len(sampled_a_features) > max_seq_len:\n",
    "            max_seq_len = len(sampled_a_features)\n",
    "    \n",
    "    # padding based on length\n",
    "    for video_struct in SEND_videos:\n",
    "        for i in range(max_seq_len-video_struct[\"seq_len\"]):\n",
    "            video_struct[\"a_feature\"].append(np.zeros(a_feature_dim))\n",
    "            video_struct[\"v_feature\"].append(np.zeros((224,224,3)))\n",
    "            video_struct[\"input_mask\"].append(0)\n",
    "\n",
    "        video_struct[\"a_feature\"] = torch.tensor(video_struct[\"a_feature\"])\n",
    "        video_struct[\"v_feature\"] = torch.tensor(video_struct[\"v_feature\"])\n",
    "        video_struct[\"input_mask\"] = torch.LongTensor(video_struct[\"input_mask\"])\n",
    "        \n",
    "    return SEND_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading all the data partitions.\n",
    "data_dir = \"../../SENDv1-data/\"\n",
    "test_modalities_data_dir = os.path.join(data_dir, \"features/Test-Hebrew/\")\n",
    "test_HEBREW_features = preprocess_HEBREW_files(\n",
    "    test_modalities_data_dir,\n",
    "    modality_dir_map=modality_dir_map,\n",
    "    preprocess=preprocess,\n",
    "    max_number_of_file=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_video_id = [video_struct[\"video_id\"] for video_struct in test_HEBREW_features]\n",
    "test_input_a_feature = torch.stack([video_struct[\"a_feature\"] for video_struct in test_HEBREW_features]).float()\n",
    "test_input_v_feature = torch.stack([video_struct[\"v_feature\"] for video_struct in test_HEBREW_features]).float()\n",
    "test_seq_lens = torch.tensor([[video_struct[\"seq_len\"]] for video_struct in test_HEBREW_features]).float()\n",
    "test_input_mask = torch.stack([video_struct[\"input_mask\"] for video_struct in test_HEBREW_features])\n",
    "test_data = TensorDataset(\n",
    "    test_input_a_feature, \n",
    "    test_input_v_feature, \n",
    "    test_seq_lens, test_input_mask\n",
    ")\n",
    "test_dataloader = DataLoader(test_data, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ablation(\n",
    "    video_id, dataloader, model, condition=\"A,V\"\n",
    "):\n",
    "    ret = {}\n",
    "    video_index = 0\n",
    "    pbar = tqdm(dataloader, desc=\"videos\")\n",
    "    for step, batch in enumerate(pbar):\n",
    "        vid_id = video_id[video_index]\n",
    "        ret[vid_id] = {}\n",
    "        # print(f\"analyzing ablation studies on video_id={vid_id}\")\n",
    "        input_a_feature, input_v_feature, seq_lens, input_mask = batch        \n",
    "        \n",
    "        # based one condition, we need to mask out some channels!\n",
    "        if \"A\" not in condition:\n",
    "            input_a_feature = torch.zeros_like(input_a_feature)\n",
    "        if \"V\" not in condition:\n",
    "            input_v_feature = torch.zeros_like(input_v_feature)\n",
    "\n",
    "        # mock linguistic and rating data.\n",
    "        batch = input_a_feature.shape[0]\n",
    "        seq_l = input_a_feature.shape[1]\n",
    "        input_l_feature = torch.zeros((batch, seq_l, 3)).long()\n",
    "        input_l_mask = torch.ones((batch, seq_l, 3)).long()\n",
    "        input_l_segment_ids = torch.zeros((batch, seq_l, 3)).long()\n",
    "        rating_labels = torch.zeros((batch, seq_l))\n",
    "            \n",
    "        _, output = \\\n",
    "            model(input_a_feature, input_l_feature, input_l_mask, input_l_segment_ids,\n",
    "                  input_v_feature, rating_labels, input_mask)\n",
    "        seq_l = int(seq_lens[0].tolist()[0])\n",
    "        pred = output[0][:seq_l].cpu().detach().numpy()\n",
    "        true = rating_labels[0][:seq_l].cpu().detach().numpy()\n",
    "        ccc = eval_ccc(pred, true)\n",
    "        ret[vid_id][\"pred\"] = pred\n",
    "        ret[vid_id][\"true\"] = true\n",
    "        video_index += 1\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "mega_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "videos:   0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyzing results for condition:  A,V\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "videos: 100%|██████████| 9/9 [03:04<00:00, 20.50s/it]\n",
      "videos:   0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyzing results for condition:  A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "videos: 100%|██████████| 9/9 [03:16<00:00, 21.80s/it]\n",
      "videos:   0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyzing results for condition:  V\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "videos: 100%|██████████| 9/9 [03:04<00:00, 20.47s/it]\n"
     ]
    }
   ],
   "source": [
    "conditions = [\"A,V\", \"A\", \"V\",]\n",
    "for condition in conditions:\n",
    "    print(\"analyzing results for condition: \", condition)\n",
    "\n",
    "    test_results = evaluate_ablation(\n",
    "        test_video_id, test_dataloader, model,\n",
    "        condition=condition\n",
    "    )\n",
    "    mega_results[condition] = {}\n",
    "    for k,v in test_results.items():\n",
    "        mega_results[condition][k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output dir:  ../data-files/target_hebrew/\n"
     ]
    }
   ],
   "source": [
    "print(\"output dir: \", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each video, we are creating a file to save ratings for all conditions.\n",
    "conditions = [\"A,V\", \"A\", \"V\",]\n",
    "for video in mega_results[\"A,V\"].keys():\n",
    "    with open(os.path.join(output_dir, f\"{video}.csv\"), \"w\") as csv_file:\n",
    "        writer = csv.writer(csv_file, delimiter=',')\n",
    "        headers = [c for c in conditions]\n",
    "        writer.writerow(headers)\n",
    "        s_len = len(mega_results[\"A,V\"][video][\"pred\"])\n",
    "        for i in range(s_len): # write line by line.\n",
    "            row = []\n",
    "            for condition in conditions:\n",
    "                norm_r = (mega_results[condition][video][\"pred\"][i]+1.0)/2.0\n",
    "                row.append(norm_r)\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
