{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Script of Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_multimodal_time_series import *\n",
    "from collections import OrderedDict\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_target_ratings=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing LinguisticEncoderBERT: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing LinguisticEncoderBERT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LinguisticEncoderBERT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the model from:  ../target/best_ccc_pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# loading model from saved model.\n",
    "model = MultimodalEmotionPrediction()\n",
    "new_state_dict = OrderedDict()\n",
    "DEVICE = torch.device('cpu')   # 'cpu' in this case\n",
    "if use_target_ratings:\n",
    "    model_path = \"../target/best_ccc_pytorch_model.bin\"\n",
    "else:\n",
    "    model_path = \"../observer/best_ccc_pytorch_model.bin\"\n",
    "print(\"loading the model from: \", model_path)\n",
    "state_dict = torch.load(model_path, map_location=DEVICE)[\"model\"]\n",
    "for k, v in state_dict.items():\n",
    "    name = k[7:] # remove `module.`\n",
    "    new_state_dict[name] = v\n",
    "model.load_state_dict(new_state_dict)\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: use_target_ratings is setting to TRUE.\n"
     ]
    }
   ],
   "source": [
    "if use_target_ratings:\n",
    "    print(\"WARNING: use_target_ratings is setting to TRUE.\")\n",
    "    modality_dir_map = {\"acoustic\": \"acoustic-egemaps\",  \n",
    "                        \"linguistic\": \"linguistic-word-level\", # we don't load features\n",
    "                        \"visual\": \"image-raw\", # image is nested,\n",
    "                        \"target\": \"target\"}\n",
    "    preprocess = {\n",
    "        'acoustic': lambda df : df.loc[:,' F0semitoneFrom27.5Hz_sma3nz_amean':' equivalentSoundLevel_dBp'],\n",
    "        'acoustic_timer': lambda df : df.loc[:,' frameTime'],\n",
    "        'linguistic': lambda df : df.loc[:,'word'],\n",
    "        'linguistic_timer': lambda df : df.loc[:,'time-offset'],\n",
    "        'target': lambda df : ((df.loc[:,' rating'] / 0.5) - 1.0),\n",
    "        'target_timer': lambda df : df.loc[:,'time'],\n",
    "    }\n",
    "else:\n",
    "    modality_dir_map = {\"acoustic\": \"acoustic-egemaps\",  \n",
    "                        \"linguistic\": \"linguistic-word-level\", # we don't load features\n",
    "                        \"visual\": \"image-raw\", # image is nested,\n",
    "                        \"target\": \"observer_EWE\"}\n",
    "    preprocess = {\n",
    "        'acoustic': lambda df : df.loc[:,' F0semitoneFrom27.5Hz_sma3nz_amean':' equivalentSoundLevel_dBp'],\n",
    "        'acoustic_timer': lambda df : df.loc[:,' frameTime'],\n",
    "        'linguistic': lambda df : df.loc[:,'word'],\n",
    "        'linguistic_timer': lambda df : df.loc[:,'time-offset'],\n",
    "        'target': lambda df : ((df.loc[:,'evaluatorWeightedEstimate'] / 50.0) - 1.0),\n",
    "        'target_timer': lambda df : df.loc[:,'time'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_target_ratings:\n",
    "    output_dir = \"../data-files/target/\"\n",
    "else:\n",
    "    output_dir = \"../data-files/observer/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    use_fast=False,\n",
    "    cache_dir=\"../.huggingface_cache/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading all the data partitions.\n",
    "data_dir = \"../../SENDv1-data/\"\n",
    "train_modalities_data_dir = os.path.join(data_dir, \"features/Train/\")\n",
    "train_target_data_dir = os.path.join(data_dir, \"ratings/Train\")\n",
    "train_SEND_features = preprocess_SEND_files(\n",
    "    train_modalities_data_dir,\n",
    "    train_target_data_dir,\n",
    "    use_target_ratings,\n",
    "    modality_dir_map=modality_dir_map,\n",
    "    preprocess=preprocess,\n",
    "    linguistic_tokenizer=tokenizer,\n",
    "    max_number_of_file=-1\n",
    ")\n",
    "\n",
    "dev_modalities_data_dir = os.path.join(data_dir, \"features/Valid/\")\n",
    "dev_target_data_dir = os.path.join(data_dir, \"ratings/Valid\")\n",
    "dev_SEND_features = preprocess_SEND_files(\n",
    "    dev_modalities_data_dir,\n",
    "    dev_target_data_dir,\n",
    "    use_target_ratings,\n",
    "    modality_dir_map=modality_dir_map,\n",
    "    preprocess=preprocess,\n",
    "    linguistic_tokenizer=tokenizer,\n",
    "    max_number_of_file=-1\n",
    ")\n",
    "\n",
    "test_modalities_data_dir = os.path.join(data_dir, \"features/Test/\")\n",
    "test_target_data_dir = os.path.join(data_dir, \"ratings/Test\")\n",
    "test_SEND_features = preprocess_SEND_files(\n",
    "    test_modalities_data_dir,\n",
    "    test_target_data_dir,\n",
    "    use_target_ratings,\n",
    "    modality_dir_map=modality_dir_map,\n",
    "    preprocess=preprocess,\n",
    "    linguistic_tokenizer=tokenizer,\n",
    "    max_number_of_file=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put dataset into correct format.\n",
    "train_video_id = [video_struct[\"video_id\"] for video_struct in train_SEND_features]\n",
    "train_input_a_feature = torch.stack([video_struct[\"a_feature\"] for video_struct in train_SEND_features]).float()\n",
    "train_input_l_feature = torch.stack([video_struct[\"l_feature\"] for video_struct in train_SEND_features])\n",
    "train_input_l_mask = torch.stack([video_struct[\"l_mask\"] for video_struct in train_SEND_features])\n",
    "train_input_l_segment_ids = torch.stack([video_struct[\"l_segment_ids\"] for video_struct in train_SEND_features])\n",
    "train_input_v_feature = torch.stack([video_struct[\"v_feature\"] for video_struct in train_SEND_features]).float()\n",
    "train_rating_labels = torch.stack([video_struct[\"rating\"] for video_struct in train_SEND_features]).float()\n",
    "train_seq_lens = torch.tensor([[video_struct[\"seq_len\"]] for video_struct in train_SEND_features]).float()\n",
    "train_input_mask = torch.stack([video_struct[\"input_mask\"] for video_struct in train_SEND_features])\n",
    "train_data = TensorDataset(\n",
    "    train_input_a_feature, \n",
    "    train_input_l_feature, train_input_l_mask, train_input_l_segment_ids,\n",
    "    train_input_v_feature, train_rating_labels, train_seq_lens, train_input_mask\n",
    ")\n",
    "train_dataloader = DataLoader(train_data, batch_size=1, shuffle=False)\n",
    "\n",
    "dev_video_id = [video_struct[\"video_id\"] for video_struct in dev_SEND_features]\n",
    "dev_input_a_feature = torch.stack([video_struct[\"a_feature\"] for video_struct in dev_SEND_features]).float()\n",
    "dev_input_l_feature = torch.stack([video_struct[\"l_feature\"] for video_struct in dev_SEND_features])\n",
    "dev_input_l_mask = torch.stack([video_struct[\"l_mask\"] for video_struct in dev_SEND_features])\n",
    "dev_input_l_segment_ids = torch.stack([video_struct[\"l_segment_ids\"] for video_struct in dev_SEND_features])\n",
    "dev_input_v_feature = torch.stack([video_struct[\"v_feature\"] for video_struct in dev_SEND_features]).float()\n",
    "dev_rating_labels = torch.stack([video_struct[\"rating\"] for video_struct in dev_SEND_features]).float()\n",
    "dev_seq_lens = torch.tensor([[video_struct[\"seq_len\"]] for video_struct in dev_SEND_features]).float()\n",
    "dev_input_mask = torch.stack([video_struct[\"input_mask\"] for video_struct in dev_SEND_features])\n",
    "dev_data = TensorDataset(\n",
    "    dev_input_a_feature, \n",
    "    dev_input_l_feature, dev_input_l_mask, dev_input_l_segment_ids,\n",
    "    dev_input_v_feature, dev_rating_labels, dev_seq_lens, dev_input_mask\n",
    ")\n",
    "dev_dataloader = DataLoader(dev_data, batch_size=1, shuffle=False)\n",
    "\n",
    "test_video_id = [video_struct[\"video_id\"] for video_struct in test_SEND_features]\n",
    "test_input_a_feature = torch.stack([video_struct[\"a_feature\"] for video_struct in test_SEND_features]).float()\n",
    "test_input_l_feature = torch.stack([video_struct[\"l_feature\"] for video_struct in test_SEND_features])\n",
    "test_input_l_mask = torch.stack([video_struct[\"l_mask\"] for video_struct in test_SEND_features])\n",
    "test_input_l_segment_ids = torch.stack([video_struct[\"l_segment_ids\"] for video_struct in test_SEND_features])\n",
    "test_input_v_feature = torch.stack([video_struct[\"v_feature\"] for video_struct in test_SEND_features]).float()\n",
    "test_rating_labels = torch.stack([video_struct[\"rating\"] for video_struct in test_SEND_features]).float()\n",
    "test_seq_lens = torch.tensor([[video_struct[\"seq_len\"]] for video_struct in test_SEND_features]).float()\n",
    "test_input_mask = torch.stack([video_struct[\"input_mask\"] for video_struct in test_SEND_features])\n",
    "test_data = TensorDataset(\n",
    "    test_input_a_feature, \n",
    "    test_input_l_feature, test_input_l_mask, test_input_l_segment_ids,\n",
    "    test_input_v_feature, test_rating_labels, test_seq_lens, test_input_mask\n",
    ")\n",
    "test_dataloader = DataLoader(test_data, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ablation(\n",
    "    video_id, dataloader, model, condition=\"A,V,L\"\n",
    "):\n",
    "    ret = {}\n",
    "    video_index = 0\n",
    "    pbar = tqdm(dataloader, desc=\"videos\")\n",
    "    for step, batch in enumerate(pbar):\n",
    "        vid_id = video_id[video_index]\n",
    "        ret[vid_id] = {}\n",
    "        # print(f\"analyzing ablation studies on video_id={vid_id}\")\n",
    "        input_a_feature, input_l_feature, input_l_mask, input_l_segment_ids, \\\n",
    "            input_v_feature, rating_labels, seq_lens, input_mask = batch\n",
    "        # based one condition, we need to mask out some channels!\n",
    "        if \"A\" not in condition:\n",
    "            input_a_feature = torch.zeros_like(input_a_feature)\n",
    "        if \"V\" not in condition:\n",
    "            input_v_feature = torch.zeros_like(input_v_feature)\n",
    "        if \"L\" not in condition:\n",
    "            input_l_feature = torch.zeros_like(input_l_feature)\n",
    "        _, output = \\\n",
    "            model(input_a_feature, input_l_feature, input_l_mask, input_l_segment_ids,\n",
    "                  input_v_feature, rating_labels, input_mask)\n",
    "        seq_l = int(seq_lens[0].tolist()[0])\n",
    "        pred = output[0][:seq_l].cpu().detach().numpy()\n",
    "        true = rating_labels[0][:seq_l].cpu().detach().numpy()\n",
    "        ccc = eval_ccc(pred, true)\n",
    "        ret[vid_id][\"pred\"] = pred\n",
    "        ret[vid_id][\"true\"] = true\n",
    "        video_index += 1\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "videos:   0%|          | 0/114 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyzing results for condition:  A,V,L\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "videos: 100%|██████████| 114/114 [04:25<00:00,  2.33s/it]\n",
      "videos: 100%|██████████| 40/40 [01:07<00:00,  1.70s/it]\n",
      "videos: 100%|██████████| 39/39 [01:15<00:00,  1.93s/it]\n",
      "videos:   0%|          | 0/114 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyzing results for condition:  A,V\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "videos: 100%|██████████| 114/114 [04:17<00:00,  2.26s/it]\n",
      "videos: 100%|██████████| 40/40 [00:59<00:00,  1.49s/it]\n",
      "videos: 100%|██████████| 39/39 [00:58<00:00,  1.51s/it]\n",
      "videos:   0%|          | 0/114 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyzing results for condition:  A,L\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "videos: 100%|██████████| 114/114 [04:06<00:00,  2.16s/it]\n",
      "videos: 100%|██████████| 40/40 [01:11<00:00,  1.78s/it]\n",
      "videos: 100%|██████████| 39/39 [01:04<00:00,  1.65s/it]\n",
      "videos:   0%|          | 0/114 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyzing results for condition:  V,L\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "videos: 100%|██████████| 114/114 [04:12<00:00,  2.22s/it]\n",
      "videos: 100%|██████████| 40/40 [01:08<00:00,  1.70s/it]\n",
      "videos: 100%|██████████| 39/39 [01:15<00:00,  1.93s/it]\n",
      "videos:   0%|          | 0/114 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyzing results for condition:  A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "videos: 100%|██████████| 114/114 [05:44<00:00,  3.02s/it]\n",
      "videos: 100%|██████████| 40/40 [01:33<00:00,  2.35s/it]\n",
      "videos: 100%|██████████| 39/39 [01:43<00:00,  2.65s/it]\n",
      "videos:   0%|          | 0/114 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyzing results for condition:  V\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "videos: 100%|██████████| 114/114 [07:04<00:00,  3.72s/it]\n",
      "videos: 100%|██████████| 40/40 [01:46<00:00,  2.66s/it]\n",
      "videos: 100%|██████████| 39/39 [01:35<00:00,  2.44s/it]\n",
      "videos:   0%|          | 0/114 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyzing results for condition:  L\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "videos: 100%|██████████| 114/114 [05:38<00:00,  2.97s/it]\n",
      "videos: 100%|██████████| 40/40 [01:31<00:00,  2.28s/it]\n",
      "videos: 100%|██████████| 39/39 [01:40<00:00,  2.58s/it]\n"
     ]
    }
   ],
   "source": [
    "conditions = [\"A,V,L\", \"A,V\", \"A,L\", \"V,L\", \"A\", \"V\", \"L\"]\n",
    "mega_results = {}\n",
    "for condition in conditions:\n",
    "    print(\"analyzing results for condition: \", condition)\n",
    "    train_results = evaluate_ablation(\n",
    "        train_video_id, train_dataloader, model,\n",
    "        condition=condition\n",
    "    )\n",
    "    \n",
    "    dev_results = evaluate_ablation(\n",
    "        dev_video_id, dev_dataloader, model,\n",
    "        condition=condition\n",
    "    )\n",
    "\n",
    "    test_results = evaluate_ablation(\n",
    "        test_video_id, test_dataloader, model,\n",
    "        condition=condition\n",
    "    )\n",
    "    mega_results[condition] = {}\n",
    "    for k,v in train_results.items():\n",
    "        mega_results[condition][k] = v\n",
    "    for k,v in dev_results.items():\n",
    "        mega_results[condition][k] = v\n",
    "    for k,v in test_results.items():\n",
    "        mega_results[condition][k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output dir:  ../data-files/target/\n"
     ]
    }
   ],
   "source": [
    "print(\"output dir: \", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each video, we are creating a file to save ratings for all conditions.\n",
    "for video in mega_results[\"A,V,L\"].keys():\n",
    "    with open(os.path.join(output_dir, f\"{video}.csv\"), \"w\") as csv_file:\n",
    "        writer = csv.writer(csv_file, delimiter=',')\n",
    "        headers = [c for c in conditions]\n",
    "        headers += [\"actual\"]\n",
    "        writer.writerow(headers)\n",
    "        s_len = len(mega_results[\"A,V,L\"][video][\"pred\"])\n",
    "        for i in range(s_len): # write line by line.\n",
    "            row = []\n",
    "            for condition in conditions:\n",
    "                norm_r = (mega_results[condition][video][\"pred\"][i]+1.0)/2.0\n",
    "                row.append(norm_r)\n",
    "            norm_r = (mega_results[condition][video][\"true\"][i]+1.0)/2.0\n",
    "            row.append(norm_r)\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data-files/train_ids.csv\", \"w\") as csv_file:\n",
    "    writer = csv.writer(csv_file, delimiter=',')\n",
    "    headers = [\"vid_id\"]\n",
    "    writer.writerow(headers)\n",
    "    for vid_id in train_video_id:\n",
    "        writer.writerow([vid_id])\n",
    "with open(\"../data-files/dev_ids.csv\", \"w\") as csv_file:\n",
    "    writer = csv.writer(csv_file, delimiter=',')\n",
    "    headers = [\"vid_id\"]\n",
    "    writer.writerow(headers)\n",
    "    for vid_id in dev_video_id:\n",
    "        writer.writerow([vid_id])\n",
    "with open(\"../data-files/test_ids.csv\", \"w\") as csv_file:\n",
    "    writer = csv.writer(csv_file, delimiter=',')\n",
    "    headers = [\"vid_id\"]\n",
    "    writer.writerow(headers)\n",
    "    for vid_id in test_video_id:\n",
    "        writer.writerow([vid_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
