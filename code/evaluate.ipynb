{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Script of Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_multimodal_time_series import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing LinguisticEncoderBERT: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.embeddings.word_embeddings.weight']\n",
      "- This IS expected if you are initializing LinguisticEncoderBERT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LinguisticEncoderBERT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LinguisticEncoderBERT were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.pretrain_word_embeddings.weight', 'bert.embeddings.study_abroad_transformation_layer.weight', 'bert.embeddings.study_abroad_transformation_layer.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# loading model from saved model.\n",
    "model = MultimodalEmotionPrediction()\n",
    "model.load_state_dict(torch.load(\"../2021-07-03-run-1/best_ccc_pytorch_model.bin\")[\"model\"])\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    use_fast=False,\n",
    "    cache_dir=\"../.huggingface_cache/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading all the data partitions.\n",
    "data_dir = \"../../SENDv1-data/\"\n",
    "train_modalities_data_dir = os.path.join(data_dir, \"features/Train/\")\n",
    "train_target_data_dir = os.path.join(data_dir, \"ratings/Train\")\n",
    "train_SEND_features = preprocess_SEND_files(\n",
    "    train_modalities_data_dir,\n",
    "    train_target_data_dir,\n",
    "    linguistic_tokenizer=tokenizer,\n",
    "    max_number_of_file=-1\n",
    ")\n",
    "\n",
    "dev_modalities_data_dir = os.path.join(data_dir, \"features/Valid/\")\n",
    "dev_target_data_dir = os.path.join(data_dir, \"ratings/Valid\")\n",
    "dev_SEND_features = preprocess_SEND_files(\n",
    "    dev_modalities_data_dir,\n",
    "    dev_target_data_dir,\n",
    "    linguistic_tokenizer=tokenizer,\n",
    "    max_number_of_file=-1\n",
    ")\n",
    "\n",
    "test_modalities_data_dir = os.path.join(data_dir, \"features/Test/\")\n",
    "test_target_data_dir = os.path.join(data_dir, \"ratings/Test\")\n",
    "test_SEND_features = preprocess_SEND_files(\n",
    "    test_modalities_data_dir,\n",
    "    test_target_data_dir,\n",
    "    linguistic_tokenizer=tokenizer,\n",
    "    max_number_of_file=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put dataset into correct format.\n",
    "train_video_id = [video_struct[\"video_id\"] for video_struct in train_SEND_features]\n",
    "train_input_a_feature = torch.stack([video_struct[\"a_feature\"] for video_struct in train_SEND_features]).float()\n",
    "train_input_l_feature = torch.stack([video_struct[\"l_feature\"] for video_struct in train_SEND_features])\n",
    "train_input_l_mask = torch.stack([video_struct[\"l_mask\"] for video_struct in train_SEND_features])\n",
    "train_input_l_segment_ids = torch.stack([video_struct[\"l_segment_ids\"] for video_struct in train_SEND_features])\n",
    "train_input_v_feature = torch.stack([video_struct[\"v_feature\"] for video_struct in train_SEND_features]).float()\n",
    "train_rating_labels = torch.stack([video_struct[\"rating\"] for video_struct in train_SEND_features]).float()\n",
    "train_seq_lens = torch.tensor([[video_struct[\"seq_len\"]] for video_struct in train_SEND_features]).float()\n",
    "train_input_mask = torch.stack([video_struct[\"input_mask\"] for video_struct in train_SEND_features])\n",
    "train_data = TensorDataset(\n",
    "    train_input_a_feature, \n",
    "    train_input_l_feature, train_input_l_mask, train_input_l_segment_ids,\n",
    "    train_input_v_feature, train_rating_labels, train_seq_lens, train_input_mask\n",
    ")\n",
    "train_dataloader = DataLoader(train_data, batch_size=1, shuffle=False)\n",
    "\n",
    "dev_video_id = [video_struct[\"video_id\"] for video_struct in dev_SEND_features]\n",
    "dev_input_a_feature = torch.stack([video_struct[\"a_feature\"] for video_struct in dev_SEND_features]).float()\n",
    "dev_input_l_feature = torch.stack([video_struct[\"l_feature\"] for video_struct in dev_SEND_features])\n",
    "dev_input_l_mask = torch.stack([video_struct[\"l_mask\"] for video_struct in dev_SEND_features])\n",
    "dev_input_l_segment_ids = torch.stack([video_struct[\"l_segment_ids\"] for video_struct in dev_SEND_features])\n",
    "dev_input_v_feature = torch.stack([video_struct[\"v_feature\"] for video_struct in dev_SEND_features]).float()\n",
    "dev_rating_labels = torch.stack([video_struct[\"rating\"] for video_struct in dev_SEND_features]).float()\n",
    "dev_seq_lens = torch.tensor([[video_struct[\"seq_len\"]] for video_struct in dev_SEND_features]).float()\n",
    "dev_input_mask = torch.stack([video_struct[\"input_mask\"] for video_struct in dev_SEND_features])\n",
    "dev_data = TensorDataset(\n",
    "    dev_input_a_feature, \n",
    "    dev_input_l_feature, dev_input_l_mask, dev_input_l_segment_ids,\n",
    "    dev_input_v_feature, dev_rating_labels, dev_seq_lens, dev_input_mask\n",
    ")\n",
    "dev_dataloader = DataLoader(dev_data, batch_size=1, shuffle=False)\n",
    "\n",
    "test_video_id = [video_struct[\"video_id\"] for video_struct in test_SEND_features]\n",
    "test_input_a_feature = torch.stack([video_struct[\"a_feature\"] for video_struct in test_SEND_features]).float()\n",
    "test_input_l_feature = torch.stack([video_struct[\"l_feature\"] for video_struct in test_SEND_features])\n",
    "test_input_l_mask = torch.stack([video_struct[\"l_mask\"] for video_struct in test_SEND_features])\n",
    "test_input_l_segment_ids = torch.stack([video_struct[\"l_segment_ids\"] for video_struct in test_SEND_features])\n",
    "test_input_v_feature = torch.stack([video_struct[\"v_feature\"] for video_struct in test_SEND_features]).float()\n",
    "test_rating_labels = torch.stack([video_struct[\"rating\"] for video_struct in test_SEND_features]).float()\n",
    "test_seq_lens = torch.tensor([[video_struct[\"seq_len\"]] for video_struct in test_SEND_features]).float()\n",
    "test_input_mask = torch.stack([video_struct[\"input_mask\"] for video_struct in test_SEND_features])\n",
    "test_data = TensorDataset(\n",
    "    test_input_a_feature, \n",
    "    test_input_l_feature, test_input_l_mask, test_input_l_segment_ids,\n",
    "    test_input_v_feature, test_rating_labels, test_seq_lens, test_input_mask\n",
    ")\n",
    "test_dataloader = DataLoader(test_data, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ablation(\n",
    "    video_id, dataloader, model,\n",
    "):\n",
    "    video_index = 0\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        vid_id = video_id[video_index]\n",
    "        print(f\"analyzing ablation studies on video_id={vid_id}\")\n",
    "        input_a_feature, input_l_feature, input_l_mask, input_l_segment_ids, \\\n",
    "            input_v_feature, rating_labels, seq_lens, input_mask = batch\n",
    "        _, output = \\\n",
    "            model(input_a_feature, input_l_feature, input_l_mask, input_l_segment_ids,\n",
    "                  input_v_feature, rating_labels, input_mask)\n",
    "        seq_l = int(seq_lens[0].tolist()[0])\n",
    "        pred = output[0][:seq_l].cpu().detach().numpy()\n",
    "        true = rating_labels[0][:seq_l].cpu().detach().numpy()\n",
    "        ccc = eval_ccc(pred, true)\n",
    "        print(ccc)\n",
    "        video_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyzing ablation studies on video_id=ID162_vid4\n",
      "-0.029260457911087293\n",
      "analyzing ablation studies on video_id=ID167_vid2\n",
      "-0.005360279659286024\n",
      "analyzing ablation studies on video_id=ID129_vid2\n",
      "-0.03959747014006752\n",
      "analyzing ablation studies on video_id=ID156_vid1\n",
      "0.06318631069985421\n",
      "analyzing ablation studies on video_id=ID178_vid5\n",
      "0.029169752435024475\n",
      "analyzing ablation studies on video_id=ID118_vid1\n",
      "-0.0017222610476933103\n",
      "analyzing ablation studies on video_id=ID172_vid2\n",
      "0.06498348697774153\n",
      "analyzing ablation studies on video_id=ID142_vid3\n",
      "0.07338304443307371\n",
      "analyzing ablation studies on video_id=ID180_vid2\n",
      "-0.016966083807458465\n",
      "analyzing ablation studies on video_id=ID147_vid5\n",
      "0.08804539993842923\n",
      "analyzing ablation studies on video_id=ID113_vid4\n",
      "-0.005784707516809227\n",
      "analyzing ablation studies on video_id=ID171_vid2\n",
      "-0.0014075871238926902\n",
      "analyzing ablation studies on video_id=ID120_vid3\n",
      "-0.028991286528435287\n",
      "analyzing ablation studies on video_id=ID179_vid1\n",
      "-0.01767528345994331\n",
      "analyzing ablation studies on video_id=ID171_vid4\n",
      "-0.013579958706233027\n",
      "analyzing ablation studies on video_id=ID114_vid6\n",
      "0.05209779344974528\n",
      "analyzing ablation studies on video_id=ID174_vid2\n",
      "0.024484598322203153\n",
      "analyzing ablation studies on video_id=ID164_vid4\n",
      "0.013291301663095705\n",
      "analyzing ablation studies on video_id=ID113_vid2\n",
      "-0.007104109230800324\n",
      "analyzing ablation studies on video_id=ID147_vid3\n",
      "-0.021842640311841674\n",
      "analyzing ablation studies on video_id=ID180_vid4\n",
      "-0.010088872505984841\n",
      "analyzing ablation studies on video_id=ID135_vid3\n",
      "0.01814403704483283\n",
      "analyzing ablation studies on video_id=ID172_vid4\n",
      "0.0035470721515425286\n",
      "analyzing ablation studies on video_id=ID144_vid3\n",
      "-0.014911004549793798\n",
      "analyzing ablation studies on video_id=ID153_vid1\n",
      "-0.03406569825552458\n",
      "analyzing ablation studies on video_id=ID121_vid1\n",
      "0.18259382727626872\n",
      "analyzing ablation studies on video_id=ID129_vid4\n",
      "0.1374598149759238\n",
      "analyzing ablation studies on video_id=ID162_vid2\n",
      "0.03222792871397367\n",
      "analyzing ablation studies on video_id=ID172_vid3\n",
      "0.08769902978366723\n",
      "analyzing ablation studies on video_id=ID121_vid6\n",
      "0.09432693615671234\n",
      "analyzing ablation studies on video_id=ID178_vid4\n",
      "0.04481751776420494\n",
      "analyzing ablation studies on video_id=ID153_vid6\n",
      "0.024922918331178524\n",
      "analyzing ablation studies on video_id=ID162_vid5\n",
      "-0.01239349908406121\n",
      "analyzing ablation studies on video_id=ID129_vid3\n",
      "-0.038817015400778275\n",
      "analyzing ablation studies on video_id=ID179_vid6\n",
      "-0.008440613715319885\n",
      "analyzing ablation studies on video_id=ID119_vid2\n",
      "-0.00990466135182773\n",
      "analyzing ablation studies on video_id=ID120_vid4\n",
      "0.11710443681374594\n",
      "analyzing ablation studies on video_id=ID174_vid5\n",
      "0.02759318984374277\n",
      "analyzing ablation studies on video_id=ID114_vid1\n",
      "-0.0517999907392645\n",
      "analyzing ablation studies on video_id=ID171_vid3\n",
      "0.06426140866817075\n",
      "analyzing ablation studies on video_id=ID113_vid5\n",
      "-0.014324044553126246\n",
      "analyzing ablation studies on video_id=ID164_vid3\n",
      "0.05952156750008939\n",
      "analyzing ablation studies on video_id=ID180_vid3\n",
      "0.0594684330219099\n",
      "analyzing ablation studies on video_id=ID142_vid2\n",
      "-0.005647636653309499\n",
      "analyzing ablation studies on video_id=ID130_vid2\n",
      "-0.020378112491756712\n",
      "analyzing ablation studies on video_id=ID147_vid4\n",
      "0.005766206019442333\n",
      "analyzing ablation studies on video_id=ID130_vid4\n",
      "-0.04537879845648632\n",
      "analyzing ablation studies on video_id=ID147_vid2\n",
      "0.010439883696485758\n",
      "analyzing ablation studies on video_id=ID180_vid5\n",
      "0.09292761320071129\n",
      "analyzing ablation studies on video_id=ID113_vid3\n",
      "0.11764214375438653\n",
      "analyzing ablation studies on video_id=ID164_vid5\n",
      "0.07115166676480197\n",
      "analyzing ablation studies on video_id=ID171_vid5\n",
      "0.07516457338623958\n",
      "analyzing ablation studies on video_id=ID111_vid1\n",
      "0.0030667668451064265\n",
      "analyzing ablation studies on video_id=ID174_vid3\n",
      "0.026762650843615097\n",
      "analyzing ablation studies on video_id=ID163_vid1\n",
      "0.02270797413795562\n",
      "analyzing ablation studies on video_id=ID119_vid4\n",
      "-0.00024068197794697117\n",
      "analyzing ablation studies on video_id=ID120_vid2\n",
      "-0.08982431416784808\n",
      "analyzing ablation studies on video_id=ID129_vid5\n",
      "0.17278104923629575\n",
      "analyzing ablation studies on video_id=ID162_vid3\n",
      "0.006925190779112158\n",
      "analyzing ablation studies on video_id=ID118_vid6\n",
      "0.0008138820665928047\n",
      "analyzing ablation studies on video_id=ID178_vid2\n",
      "0.0014249515405871096\n",
      "analyzing ablation studies on video_id=ID156_vid6\n",
      "-0.12107330234586106\n",
      "analyzing ablation studies on video_id=ID151_vid2\n",
      "-0.05187622192153899\n",
      "analyzing ablation studies on video_id=ID172_vid5\n",
      "0.00459545086097807\n",
      "analyzing ablation studies on video_id=ID111_vid4\n",
      "0.08798328073810438\n",
      "analyzing ablation studies on video_id=ID174_vid6\n",
      "0.0017416527151452844\n",
      "analyzing ablation studies on video_id=ID179_vid5\n",
      "-0.016274222716335624\n",
      "analyzing ablation studies on video_id=ID130_vid1\n",
      "0.11960820575108543\n",
      "analyzing ablation studies on video_id=ID142_vid1\n",
      "-0.0337576431770797\n",
      "analyzing ablation studies on video_id=ID113_vid6\n",
      "0.17973639586613174\n",
      "analyzing ablation studies on video_id=ID143_vid3\n",
      "0.11818210327679267\n",
      "analyzing ablation studies on video_id=ID162_vid6\n",
      "0.041959036590958676\n",
      "analyzing ablation studies on video_id=ID153_vid5\n",
      "-0.04754736599684133\n",
      "analyzing ablation studies on video_id=ID121_vid5\n",
      "0.09828438342016937\n",
      "analyzing ablation studies on video_id=ID118_vid3\n",
      "-0.017115708333155167\n",
      "analyzing ablation studies on video_id=ID156_vid3\n",
      "-0.0008191159614451274\n",
      "analyzing ablation studies on video_id=ID141_vid1\n",
      "-0.014634651881258261\n",
      "analyzing ablation studies on video_id=ID121_vid3\n",
      "-0.05274356035138611\n",
      "analyzing ablation studies on video_id=ID118_vid5\n",
      "-0.014042458281186438\n",
      "analyzing ablation studies on video_id=ID178_vid1\n",
      "-0.031455425999206386\n",
      "analyzing ablation studies on video_id=ID144_vid1\n",
      "0.03338435408672594\n",
      "analyzing ablation studies on video_id=ID153_vid3\n",
      "-0.048477871088767055\n",
      "analyzing ablation studies on video_id=ID129_vid6\n",
      "0.12724900389886964\n",
      "analyzing ablation studies on video_id=ID143_vid5\n",
      "0.05511429468254751\n",
      "analyzing ablation studies on video_id=ID180_vid6\n",
      "-0.034217683774749895\n",
      "analyzing ablation studies on video_id=ID147_vid1\n",
      "-0.030908131487794154\n",
      "analyzing ablation studies on video_id=ID179_vid3\n",
      "-0.00920466828198893\n",
      "analyzing ablation studies on video_id=ID120_vid1\n",
      "0.0006186397934243887\n",
      "analyzing ablation studies on video_id=ID163_vid2\n",
      "-0.00817722838092873\n",
      "analyzing ablation studies on video_id=ID114_vid4\n",
      "0.02795733940436487\n",
      "analyzing ablation studies on video_id=ID171_vid6\n",
      "-0.031207154861644226\n",
      "analyzing ablation studies on video_id=ID169_vid2\n",
      "0.2749117374164573\n",
      "analyzing ablation studies on video_id=ID180_vid1\n",
      "-0.016527285915968935\n",
      "analyzing ablation studies on video_id=ID179_vid4\n",
      "-0.0030998321332647515\n",
      "analyzing ablation studies on video_id=ID171_vid1\n",
      "-0.011856814639223077\n",
      "analyzing ablation studies on video_id=ID163_vid5\n",
      "-0.021973244644008908\n",
      "analyzing ablation studies on video_id=ID174_vid7\n",
      "0.050720450555086664\n",
      "analyzing ablation studies on video_id=ID153_vid4\n",
      "0.08220988844994363\n",
      "analyzing ablation studies on video_id=ID156_vid2\n",
      "-0.0016262902652879278\n",
      "analyzing ablation studies on video_id=ID178_vid6\n",
      "-0.04079249337055072\n",
      "analyzing ablation studies on video_id=ID167_vid1\n",
      "-0.03419618236019516\n",
      "analyzing ablation studies on video_id=ID129_vid1\n",
      "0.009926382848661788\n",
      "analyzing ablation studies on video_id=ID172_vid1\n",
      "0.17047492773614067\n",
      "analyzing ablation studies on video_id=ID151_vid6\n",
      "0.05579226354016284\n",
      "analyzing ablation studies on video_id=ID129_vid7\n",
      "0.06170948896410377\n",
      "analyzing ablation studies on video_id=ID141_vid6\n",
      "0.08268644462013174\n",
      "analyzing ablation studies on video_id=ID118_vid4\n",
      "0.1903763525269427\n",
      "analyzing ablation studies on video_id=ID153_vid2\n",
      "0.07127258484117883\n",
      "analyzing ablation studies on video_id=ID114_vid5\n",
      "0.004916731920655135\n",
      "analyzing ablation studies on video_id=ID174_vid1\n",
      "0.043190648854618936\n",
      "analyzing ablation studies on video_id=ID111_vid3\n",
      "0.004337092048367916\n",
      "analyzing ablation studies on video_id=ID119_vid6\n",
      "0.031065190818787294\n",
      "analyzing ablation studies on video_id=ID169_vid4\n",
      "0.2082563138930841\n",
      "analyzing ablation studies on video_id=ID130_vid6\n",
      "0.21024610704972865\n"
     ]
    }
   ],
   "source": [
    "_ = evaluate_ablation(train_video_id, train_dataloader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
