{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after process data structure for each modality\n",
    "# acoustic = [[[feature at time 1], [feature at time 2], [...]], [...]]\n",
    "# linguistic = [[[token_1, token_2, ... at time 1], [token_1, token_2, ... at time 2], [...]], [...]]\n",
    "# visual = [[[feature at time 1], [feature at time 2], [...]], [...]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "modalities_data_dir = \"../../SENDv1-data/features/Train/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = {\n",
    "    'acoustic': lambda df : df.loc[:,' F0semitoneFrom27.5Hz_sma3nz_amean':' equivalentSoundLevel_dBp'],\n",
    "    'acoustic_timer': lambda df : df.loc[:,' frameTime'],\n",
    "    'linguistic': lambda df : df.loc[:,'word'],\n",
    "    'linguistic_timer': lambda df : df.loc[:,'time-offset'],\n",
    "}\n",
    "\n",
    "class InputFeature:\n",
    "    \n",
    "    def __init__(\n",
    "        self, video_id=\"\",\n",
    "        acoustic_feature=[],\n",
    "        linguistic_feature=[],\n",
    "        visual_feature=[],\n",
    "        labels=[],\n",
    "    ):\n",
    "        self.video_id = video_id\n",
    "        self.acoustic_feature = acoustic_feature\n",
    "        self.linguistic_feature = linguistic_feature\n",
    "        self.visual_feature = visual_feature\n",
    "        self.labels = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_SEND_files(\n",
    "    data_dir,\n",
    "    time_window_in_sec=5.0,\n",
    "    modality_dir_map = {\"acoustic\": \"acoustic-egemaps\",  \n",
    "                        \"linguistic\": \"linguistic-word-level\", # we don't load features\n",
    "                        \"visual\": \"image-raw\", # image is nested\n",
    "                       },\n",
    "    keep_first=True,\n",
    "    linguistic_tokenizer=None,\n",
    "):\n",
    "    # basically, let us gett all the video ids?\n",
    "    a_ids = [f.split(\"_\")[0]+\"_\"+f.split(\"_\")[1] \n",
    "             for f in listdir(os.path.join(data_dir, modality_dir_map[\"acoustic\"])) \n",
    "             if isfile(os.path.join(data_dir, modality_dir_map[\"acoustic\"], f))]\n",
    "    l_ids = [f.split(\"_\")[0]+\"_\"+f.split(\"_\")[1] \n",
    "             for f in listdir(os.path.join(data_dir, modality_dir_map[\"linguistic\"])) \n",
    "             if isfile(os.path.join(data_dir, modality_dir_map[\"linguistic\"], f))]\n",
    "    v_ids = [f.split(\"_\")[0]+\"_\"+f.split(\"_\")[1] \n",
    "             for f in listdir(os.path.join(data_dir, modality_dir_map[\"visual\"])) \n",
    "             if f != \".DS_Store\"]\n",
    "    assert len(a_ids) == len(l_ids) and len(l_ids) == len(v_ids)\n",
    "    assert len(set(a_ids).intersection(set(l_ids))) == len(l_ids)\n",
    "    assert len(set(a_ids).intersection(set(v_ids))) == len(v_ids)\n",
    "    \n",
    "    # We need the first pass for linguistic modality process?\n",
    "    \n",
    "    for video_id in a_ids: # pick any one!\n",
    "        \n",
    "        # acoustic features process\n",
    "        a_file = os.path.join(data_dir, modality_dir_map[\"acoustic\"], f\"{video_id}_acousticFeatures.csv\")\n",
    "        a_df = pd.read_csv(a_file)\n",
    "        a_features = np.array(preprocess[\"acoustic\"](a_df))\n",
    "        a_timestamps = np.array(preprocess[\"acoustic_timer\"](a_df))\n",
    "        assert a_features.shape[0] == a_timestamps.shape[0]\n",
    "        # sample based on interval\n",
    "        current_time = 0.0\n",
    "        keep_first = keep_first\n",
    "        sampled_a_features = []\n",
    "        sampled_a_timestamps  = []\n",
    "        for i in range(0, a_timestamps.shape[0]):\n",
    "            if keep_first:\n",
    "                sampled_a_features += [a_features[i]]\n",
    "                sampled_a_timestamps+= [a_timestamps[i]]\n",
    "                keep_first = False\n",
    "            if a_timestamps[i] >= current_time+time_window_in_sec:\n",
    "                sampled_a_features += [a_features[i]]\n",
    "                sampled_a_timestamps+= [a_timestamps[i]]\n",
    "                current_time += time_window_in_sec\n",
    "        sampled_a_features = np.array(sampled_a_features)\n",
    "        sampled_a_timestamps = np.array(sampled_a_timestamps)\n",
    "        assert sampled_a_features.shape[0] == sampled_a_timestamps.shape[0]\n",
    "        \n",
    "        # linguistic features process\n",
    "        l_file = os.path.join(data_dir, modality_dir_map[\"linguistic\"], f\"{video_id}_aligned.tsv\")\n",
    "        l_df = pd.read_csv(l_file, sep='\\t')\n",
    "        l_words = np.array(preprocess[\"linguistic\"](l_df))\n",
    "        l_words = [w.strip().lower() for w in l_words]\n",
    "        l_timestamps = np.array(preprocess[\"linguistic_timer\"](l_df))\n",
    "        assert len(l_words) == l_timestamps.shape[0]\n",
    "        # sample based on interval\n",
    "        current_time = 0.0\n",
    "        keep_first = keep_first\n",
    "        sampled_l_words = [] # different from other modality, it is essentially a list of list!\n",
    "        sampled_l_timestamps = []\n",
    "        tmp_words = []\n",
    "        for i in range(0, l_timestamps.shape[0]):\n",
    "            if keep_first:\n",
    "                sampled_l_words += [[]]\n",
    "                sampled_l_timestamps += [0.0]\n",
    "                keep_first = False\n",
    "            if l_timestamps[i] >= current_time+time_window_in_sec:\n",
    "                sampled_l_words.append(tmp_words)\n",
    "                this_timestampe = current_time+time_window_in_sec\n",
    "                sampled_l_timestamps += [this_timestampe]\n",
    "                tmp_words = [l_words[i]] # reinit the buffer\n",
    "                current_time += time_window_in_sec\n",
    "                continue\n",
    "            tmp_words += [l_words[i]]\n",
    "        # overflow\n",
    "        if len(tmp_words) > 0:\n",
    "            sampled_l_words.append(tmp_words)\n",
    "            this_timestampe = current_time+time_window_in_sec\n",
    "            sampled_l_timestamps += [this_timestampe]\n",
    "        sampled_l_timestamps = np.array(sampled_l_timestamps)\n",
    "        sampled_l_token_ids = []\n",
    "        for window_words in sampled_l_words:\n",
    "            token_ids = linguistic_tokenizer.convert_tokens_to_ids(window_words)\n",
    "            sampled_l_token_ids += [token_ids]\n",
    "        assert len(sampled_l_words) == sampled_l_timestamps.shape[0]\n",
    "        assert len(sampled_l_token_ids) == sampled_l_timestamps.shape[0]\n",
    "        print(sampled_l_token_ids)\n",
    "        \n",
    "        # visual features process\n",
    "        # for visual, we actually need to active control what image we load, we\n",
    "        # cannot just load all images, it will below memory.\n",
    "        fps=30 # We may need to dynamically figure out this number?\n",
    "        frame_names = []\n",
    "        for f in listdir(os.path.join(data_dir, modality_dir_map[\"visual\"], video_id)):\n",
    "            if \".jpg\" in f:\n",
    "                frame_names += [(int(f.split(\"_\")[0][5:])*(1.0/fps), f)]\n",
    "        frame_names.sort(key=lambda x:x[0])\n",
    "        sampled_frames = []\n",
    "        current_time = 0.0\n",
    "        keep_first = keep_first\n",
    "        for f in frame_names:\n",
    "            if keep_first:\n",
    "                sampled_frames += [f]\n",
    "                keep_first = False\n",
    "            if f[0] >= current_time+time_window_in_sec:\n",
    "                sampled_frames += [f]\n",
    "                current_time += time_window_in_sec\n",
    "        v_images = []\n",
    "        v_timestamps = []\n",
    "        for f in sampled_frames:\n",
    "            f_path = os.path.join(data_dir, modality_dir_map[\"visual\"], video_id, f[1])\n",
    "            f_image = Image.open(f_path)\n",
    "            f_data = asarray(f_image)\n",
    "            v_images += [f_data] \n",
    "            v_timestamps += [f[0]]\n",
    "        v_images = np.array(v_images)\n",
    "        v_timestamps = np.array(v_timestamps)\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    use_fast=False,\n",
    "    cache_dir=\"../.huggingface_cache/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2043, 1045, 2001, 7763, 100, 2034, 2051, 1045, 2001, 1999, 2293], [2005, 100, 1045, 2228, 1045, 2196, 11333, 100, 2196, 2001, 1999, 2293], [2077, 100, 2008, 2001, 1996, 2034, 2051, 1045, 2001, 1999, 2293, 2007, 2619, 2008, 2001, 2066, 100, 1045], [3984, 2116, 2111, 2113, 2008, 100, 1045, 3246, 2017, 100], [1045, 2074, 2514, 2066, 100, 2061, 100], [1997, 2673, 2003, 2061, 100, 2066, 2074, 2156, 2008, 2711, 2066, 2045, 2017, 100, 2115], [2540, 2003, 2066, 6012, 1037, 3634, 2335, 1037, 2117], [1998, 100, 2017, 2514, 2061, 100, 100, 1045, 100, 100, 100, 2066, 100, 2017, 100], [1996, 3110, 1997, 100, 100], [100, 2055, 1037, 100, 2066, 100, 2200, 100, 2138, 1045, 2001, 2061, 100, 1045, 2001, 2014, 2767], [100, 1045, 2001, 2066, 100, 1045, 2196, 2371, 1999, 2293, 2007, 2619, 2030, 2018, 1037, 3276], [2077, 100, 2061, 2009, 2001, 2785, 1997, 100, 100, 2066, 100, 3100, 1045, 2064], [2991, 1999, 3062, 1045, 1045, 2064, 2991, 1999, 2293, 100, 1998], [2031, 2619, 1999, 2026, 2166, 2008, 2001, 100, 100, 100], [100, 4606, 1996, 2711, 2001, 2200, 3835, 1998, 100], [100, 100, 2061, 100], [3893, 100, 2130, 2295, 2009, 2001, 8552, 3276, 100, 2059], [2001, 2428, 100, 1045, 3030, 1042, 1045, 2001, 1037, 2200, 2204, 2066], [2152, 2082, 3076, 2001, 2066, 2028, 1997, 1996, 2190, 1999, 2026, 2465, 100], [1998, 1998, 3402, 1045, 3030, 11922, 2055, 2066, 2151, 2118, 2030], [100, 1045, 2074, 2066, 4711, 3294, 23657, 2013, 100], [1998, 2476, 1998, 100, 100, 2066, 2005, 2048, 2086, 2061, 2028, 2095, 1045], [2106, 2066, 1037, 100, 6578, 2096, 1045, 6158, 1037], [2978, 1998, 1045, 2001, 4208, 2006, 2026, 3276, 1998, 1996, 2117, 2095, 1045, 2253, 2000, 2047, 2259, 100], [1998, 100, 1045, 3984, 1045, 2785, 1997, 3478, 2138, 2008], [100, 2025, 1037, 3634, 3867, 2138, 1997, 2008, 3276, 2138], [2009, 2001, 2205, 2205, 2116, 100, 2009, 2001, 2066], [2635, 2066, 13568, 3867, 1997, 2026, 100, 2066, 2008, 2001, 2205], [100, 2205, 2172, 6699, 1998, 3398], [100, 1045, 100, 100, 2021, 100, 100, 2061, 2008, 2001, 2026, 2034], [100, 1045, 100, 100]]\n"
     ]
    }
   ],
   "source": [
    "preprocess_SEND_files(\n",
    "    modalities_data_dir,\n",
    "    linguistic_tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
