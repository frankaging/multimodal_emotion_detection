{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after process data structure for each modality\n",
    "# acoustic = [[[feature at time 1], [feature at time 2], [...]], [...]]\n",
    "# linguistic = [[[token_1, token_2, ... at time 1], [token_1, token_2, ... at time 2], [...]], [...]]\n",
    "# visual = [[[feature at time 1], [feature at time 2], [...]], [...]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "modalities_data_dir = \"../../SENDv1-data/features/Train/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = {\n",
    "    'acoustic': lambda df : df.loc[:,' F0semitoneFrom27.5Hz_sma3nz_amean':' equivalentSoundLevel_dBp'],\n",
    "    'acoustic_timer': lambda df : df.loc[:,' frameTime'],\n",
    "    'linguistic': lambda df : df.loc[:,'word'],\n",
    "    'linguistic_timer': lambda df : df.loc[:,'time-offset'],\n",
    "}\n",
    "\n",
    "class InputFeature:\n",
    "    \n",
    "    def __init__(\n",
    "        self, video_id=\"\",\n",
    "        acoustic_feature=[],\n",
    "        linguistic_feature=[],\n",
    "        visual_feature=[],\n",
    "        labels=[],\n",
    "    ):\n",
    "        self.video_id = video_id\n",
    "        self.acoustic_feature = acoustic_feature\n",
    "        self.linguistic_feature = linguistic_feature\n",
    "        self.visual_feature = visual_feature\n",
    "        self.labels = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_SEND_files(\n",
    "    data_dir,\n",
    "    time_window_in_sec=5.0,\n",
    "    modality_dir_map = {\"acoustic\": \"acoustic-egemaps\",  \n",
    "                        \"linguistic\": \"linguistic-word-level\", # we don't load features\n",
    "                        \"visual\": \"image-raw\", # image is nested\n",
    "                       },\n",
    "    keep_first=True,\n",
    "):\n",
    "    # basically, let us gett all the video ids?\n",
    "    a_ids = [f.split(\"_\")[0]+\"_\"+f.split(\"_\")[1] \n",
    "             for f in listdir(os.path.join(data_dir, modality_dir_map[\"acoustic\"])) \n",
    "             if isfile(os.path.join(data_dir, modality_dir_map[\"acoustic\"], f))]\n",
    "    l_ids = [f.split(\"_\")[0]+\"_\"+f.split(\"_\")[1] \n",
    "             for f in listdir(os.path.join(data_dir, modality_dir_map[\"linguistic\"])) \n",
    "             if isfile(os.path.join(data_dir, modality_dir_map[\"linguistic\"], f))]\n",
    "    v_ids = [f.split(\"_\")[0]+\"_\"+f.split(\"_\")[1] \n",
    "             for f in listdir(os.path.join(data_dir, modality_dir_map[\"visual\"])) \n",
    "             if f != \".DS_Store\"]\n",
    "    assert len(a_ids) == len(l_ids) and len(l_ids) == len(v_ids)\n",
    "    assert len(set(a_ids).intersection(set(l_ids))) == len(l_ids)\n",
    "    assert len(set(a_ids).intersection(set(v_ids))) == len(v_ids)\n",
    "    \n",
    "    for video_id in a_ids: # pick any one!\n",
    "        \n",
    "        # acoustic features process\n",
    "        a_file = os.path.join(data_dir, modality_dir_map[\"acoustic\"], f\"{video_id}_acousticFeatures.csv\")\n",
    "        a_df = pd.read_csv(a_file)\n",
    "        a_features = np.array(preprocess[\"acoustic\"](a_df))\n",
    "        a_timestamps = np.array(preprocess[\"acoustic_timer\"](a_df))\n",
    "        # sample based on interval\n",
    "        current_time = 0.0\n",
    "        keep_first = keep_first\n",
    "        sampled_a_features = []\n",
    "        sampled_a_timestamps  = []\n",
    "        \n",
    "        \n",
    "        # linguistic features process\n",
    "        l_file = os.path.join(data_dir, modality_dir_map[\"linguistic\"], f\"{video_id}_aligned.tsv\")\n",
    "        l_df = pd.read_csv(l_file, sep='\\t')\n",
    "        l_words = np.array(preprocess[\"linguistic\"](l_df))\n",
    "        l_words = [w.strip().lower() for w in l_words]\n",
    "        l_timestamps = np.array(preprocess[\"linguistic_timer\"](l_df))\n",
    "        # sample based on interval\n",
    "        current_time = 0.0\n",
    "        keep_first = keep_first\n",
    "        sampled_l_words = [] # different from other modality, it is essentially a list of list!\n",
    "        sampled_l_timestamps = []\n",
    "        \n",
    "        \n",
    "        # visual features process\n",
    "        # for visual, we actually need to active control what image we load, we\n",
    "        # cannot just load all images, it will below memory.\n",
    "        fps=30 # We may need to dynamically figure out this number?\n",
    "        frame_names = []\n",
    "        for f in listdir(os.path.join(data_dir, modality_dir_map[\"visual\"], video_id)):\n",
    "            if \".jpg\" in f:\n",
    "                frame_names += [(int(f.split(\"_\")[0][5:])*(1.0/fps), f)]\n",
    "        frame_names.sort(key=lambda x:x[0])\n",
    "        sampled_frames = []\n",
    "        current_time = 0.0\n",
    "        keep_first = keep_first\n",
    "        for f in frame_names:\n",
    "            if keep_first:\n",
    "                sampled_frames += [f]\n",
    "                keep_first = False\n",
    "            if f[0] >= current_time+time_window_in_sec:\n",
    "                sampled_frames += [f]\n",
    "                current_time += time_window_in_sec\n",
    "        v_images = []\n",
    "        v_timestamps = []\n",
    "        for f in sampled_frames:\n",
    "            f_path = os.path.join(data_dir, modality_dir_map[\"visual\"], video_id, f[1])\n",
    "            f_image = Image.open(f_path)\n",
    "            f_data = asarray(f_image)\n",
    "            v_images += [f_data] \n",
    "            v_timestamps += [f[0]]\n",
    "        v_images = np.array(v_images)\n",
    "        v_timestamps = np.array(v_timestamps)\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(308,)\n"
     ]
    }
   ],
   "source": [
    "preprocess_SEND_files(modalities_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
